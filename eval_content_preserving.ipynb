{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the model name and similarity metric to the model you want to use\n",
    "model_name=\"blip\"\n",
    "sim_metric='ssim' #'llava' or 'blip' or 'ssim'\n",
    "####################################\n",
    "\n",
    "base_path = \"output/model\"\n",
    "constraints = {\n",
    "    \"Positive\": \"Sentiment/Positive/constrained_eps_32_batch_8\",\n",
    "    \"Negative\": \"Sentiment/Negative/constrained_eps_32_batch_8\",\n",
    "    \"Neutral\": \"Sentiment/Neutral/constrained_eps_32_batch_8\",\n",
    "    \"Formal\": \"Formality/Formal/constrained_eps_32_batch_8\",\n",
    "    \"Informal\": \"Formality/Informal/constrained_eps_32_batch_8\",\n",
    "    \"English\": \"Language/English/constrained_eps_32_batch_8\",\n",
    "    \"French\": \"Language/French/constrained_eps_32_batch_8\",\n",
    "    \"Spanish\": \"Language/Spanish/constrained_eps_32_batch_8\",\n",
    "    \"Left\": \"Politics/Left/constrained_eps_32_batch_8\",\n",
    "    \"Right\": \"Politics/Right/constrained_eps_32_batch_8\",\n",
    "    \"Spam\": \"Attack/Spam/constrained_eps_32_batch_8\",\n",
    "    \"Injection\": \"Attack/Injection/constrained_eps_32_batch_8\"\n",
    "}\n",
    "\n",
    "file_dict = {}\n",
    "for key, constraint in constraints.items():\n",
    "    # First add paths with format base_path/i/constraint/\n",
    "    paths = [f\"{base_path}/{i}/{constraint}/\" for i in range(0,5)]\n",
    "    # Then add paths with format base_path/coco_i/constraint/\n",
    "    paths.extend([f\"{base_path}/coco_{i}/{constraint}/\" for i in range(1,11)])\n",
    "    file_dict[key] = paths\n",
    "\n",
    "\n",
    "if model_name == 'minigpt4':\n",
    "    for category, paths in file_dict.items():\n",
    "        file_dict[category] = [path.replace(\"model\", model_name) for path in paths]\n",
    "elif model_name == 'llava':\n",
    "    for category, paths in file_dict.items():\n",
    "        file_dict[category] = [path.replace(\"model\", model_name) for path in paths]\n",
    "elif model_name == 'blip':\n",
    "    for category, paths in file_dict.items():\n",
    "        file_dict[category] = [path.replace(\"model\", model_name) for path in paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classfication: To check if the model's output matches the label/content of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def get_classification_result(file_path):\n",
    "    outputs = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            if 'output' in data:\n",
    "                outputs.append(data['output'])\n",
    "    \n",
    "    yes_count = sum(1 for o in outputs if \"yes\" in o.lower() or \"oui\" in o.lower())\n",
    "    total = len(outputs)\n",
    "    return yes_count/total if total > 0 else 0\n",
    "\n",
    "classification_dict = {}\n",
    "for key in file_dict:\n",
    "    results = [get_classification_result(f\"{file}content_classification_result.jsonl\") for file in file_dict[key]]\n",
    "    classification_dict[key] = {\n",
    "        \"mean\": round(np.mean(results), 2),\n",
    "        \"std\": round(np.std(results), 2)\n",
    "    }\n",
    "\n",
    "# Calculate category means\n",
    "categories = {\n",
    "    \"sentiment\": [\"Positive\", \"Negative\", \"Neutral\"],\n",
    "    \"language\": [\"English\", \"French\", \"Spanish\"],\n",
    "    \"formality\": [\"Formal\", \"Informal\"],\n",
    "    \"politics\": [\"Left\", \"Right\"],\n",
    "    \"Attack\": [\"Spam\", \"Injection\"]\n",
    "}\n",
    "\n",
    "for cat, keys in categories.items():\n",
    "    means = [classification_dict[k][\"mean\"] for k in keys]\n",
    "    stds = [classification_dict[k][\"std\"] for k in keys]\n",
    "    classification_dict[cat] = {\n",
    "        \"mean\": round(sum(means)/len(means), 2),\n",
    "        \"std\": round(np.sqrt(sum(s**2 for s in stds)/len(stds)), 2)\n",
    "    }\n",
    "\n",
    "classification_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity between oringinal and perturbed image\n",
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization and Embedding Calculation\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "if model_name == \"llava\":\n",
    "    import argparse\n",
    "    from llava_llama_2.utils import get_model\n",
    "    \n",
    "    def parse_args():\n",
    "        parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "        parser.add_argument(\"--model_path\", type=str, default=\"./ckpts/llava_llama_2_13b_chat_freeze\")\n",
    "        parser.add_argument(\"--gpu_id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "        parser.add_argument(\"--model_base\", type=str, default=None)\n",
    "        return parser.parse_args(args=['--gpu_id', '0'])\n",
    "\n",
    "    args = parse_args()\n",
    "    tokenizer, model, image_processor, model_name = get_model(args)\n",
    "    model.eval()\n",
    "\n",
    "elif model_name == \"minigpt4\":\n",
    "    import argparse\n",
    "    from minigpt4.common.config import Config\n",
    "    from minigpt4.common.registry import registry\n",
    "    from minigpt_utils import prompt_wrapper, generator\n",
    "    \n",
    "    def parse_args():\n",
    "        parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "        parser.add_argument(\"--cfg-path\", default=\"eval_configs/minigpt4_eval.yaml\", help=\"path to configuration file.\")\n",
    "        parser.add_argument(\"--gpu-id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "        parser.add_argument(\"--mode\", type=str, default='VisualChatBot', choices=[\"TextOnly\", \"VisualChatBot\"])\n",
    "        parser.add_argument(\"--image_file\", type=str, default='./image.bmp', help=\"Image file\")\n",
    "        parser.add_argument(\"--output_file\", type=str, default='./result.jsonl', help=\"Output file.\")\n",
    "        parser.add_argument(\"--options\", nargs=\"+\")\n",
    "        return parser.parse_args(args=['--cfg-path', 'eval_configs/minigpt4_eval.yaml', \n",
    "                                     '--gpu-id', '0', '--mode', 'VisualChatBot', \n",
    "                                     '--image_file', 'clean_images/0.png', \n",
    "                                     '--output_file', './result.jsonl'])\n",
    "    args = parse_args()\n",
    "    cfg = Config(args)\n",
    "    model_config = cfg.model_cfg\n",
    "    model_config.device_8bit = args.gpu_id\n",
    "    model = registry.get_model_class(model_config.arch).from_config(model_config).to(f'cuda:{args.gpu_id}')\n",
    "    vis_processor = registry.get_processor_class(cfg.datasets_cfg.cc_sbu_align.vis_processor.train.name).from_config(cfg.datasets_cfg.cc_sbu_align.vis_processor.train)\n",
    "    my_generator = generator.Generator(model=model)\n",
    "\n",
    "elif model_name == \"blip\":\n",
    "    import argparse\n",
    "    from lavis.models import load_model_and_preprocess\n",
    "    \n",
    "    def parse_args():\n",
    "        parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "        parser.add_argument(\"--gpu-id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "        parser.add_argument(\"--data_path\", type=str, default=\"instruction_data/0/Sentiment/dataset.csv\")\n",
    "        parser.add_argument(\"--image_file\", type=str, default='./image.bmp', help=\"Image file\")\n",
    "        parser.add_argument(\"--output_file\", type=str, default='./result.jsonl', help=\"Output file.\")\n",
    "        parser.add_argument(\"--instruction\", type=str, default=None, choices=[\"positive\", \"negative\", \"neutral\", \"irony\", \"non_irony\", \"formal\", \"informal\", \"french\", \"english\", \"spanish\", \"left\", \"right\", \"inference_content_evaluation\", \"injection\", \"spam\"])\n",
    "        parser.add_argument(\"--image_index\", type=int, default=0)\n",
    "        return parser.parse_args(args=['--data_path', 'instruction_data/0/Attack/dataset.csv',\n",
    "                                     '--image_file', 'clean_images/0.png',\n",
    "                                     '--output_file', './result.jsonl'])\n",
    "\n",
    "    args = parse_args()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model, vis_processor, _ = load_model_and_preprocess(\n",
    "        name='blip2_vicuna_instruct',\n",
    "        model_type='vicuna13b',\n",
    "        is_eval=True,\n",
    "        device=device\n",
    "    )\n",
    "    model.eval()\n",
    "    img = Image.open(args.image_file).convert('RGB')\n",
    "    img = vis_processor[\"eval\"](img).unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_msssim import ssim\n",
    "\n",
    "def denormalize(images):\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()\n",
    "    return images * std[None, :, None, None] + mean[None, :, None, None]\n",
    "\n",
    "def load_image(image_path):\n",
    "    return Image.open(image_path).convert('RGB')\n",
    "\n",
    "def calculate_similarity(image1, image2, model_name, model, sim_metric):\n",
    "    cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    \n",
    "    # Process images if not already tensors\n",
    "    if not isinstance(image1, torch.Tensor):\n",
    "        if model_name == 'llava':\n",
    "            image1 = image_processor.preprocess(image1, return_tensors='pt')['pixel_values'].cuda()\n",
    "            image2 = image_processor.preprocess(image2, return_tensors='pt')['pixel_values'].cuda()\n",
    "        elif model_name == 'minigpt4':\n",
    "            image1 = [vis_processor(image1).unsqueeze(0).to(model.device)]\n",
    "            image2 = [vis_processor(image2).unsqueeze(0).to(model.device)]\n",
    "        elif model_name == 'blip':\n",
    "            image1 = vis_processor[\"eval\"](image1).unsqueeze(0).to(device)\n",
    "            image2 = vis_processor[\"eval\"](image2).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get embeddings based on model type\n",
    "    if sim_metric == 'llava':\n",
    "        emb1 = model.encode_images(image1.half())\n",
    "        emb2 = model.encode_images(image2.half())\n",
    "    elif sim_metric == 'minigpt4':\n",
    "        prompt1 = prompt_wrapper.Prompt(model=model, img_prompts=[[image1]])\n",
    "        prompt2 = prompt_wrapper.Prompt(model=model, img_prompts=[[image2]])\n",
    "        emb1, emb2 = prompt1.img_embs[0][0], prompt2.img_embs[0][0]\n",
    "    elif sim_metric == 'blip':\n",
    "        with model.maybe_autocast():\n",
    "            emb1 = model.ln_vision(model.visual_encoder(image1))\n",
    "            emb2 = model.ln_vision(model.visual_encoder(image2))\n",
    "    elif sim_metric == 'ssim':\n",
    "        return ssim(image1, image2, data_range=1.0, size_average=True)\n",
    "    return cos(emb1.view(-1).to(torch.float32), emb2.view(-1).to(torch.float32))\n",
    "\n",
    "def calculate_transformations_sim(image, model_name, model, sim_metric):\n",
    "    transforms_list = [\n",
    "        transforms.GaussianBlur(9, sigma=(1.0, 5.0)),\n",
    "        transforms.RandomAffine(45),\n",
    "        transforms.ColorJitter(0.5, 0.5, 0.5, 0.2),\n",
    "        transforms.RandomHorizontalFlip(1),\n",
    "        transforms.RandomPerspective(0.5, 1)\n",
    "    ]\n",
    "    # Process image based on model\n",
    "    if model_name == 'llava':\n",
    "        image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].cuda()\n",
    "    elif model_name == 'minigpt4':\n",
    "        image_tensor = vis_processor(image).unsqueeze(0).cuda()\n",
    "    elif model_name == 'blip':\n",
    "        image_tensor = vis_processor[\"eval\"](image).unsqueeze(0).cuda()\n",
    "\n",
    "    # Calculate similarities for each transformation\n",
    "    sims = [calculate_similarity(\n",
    "        denormalize(image_tensor), \n",
    "        denormalize(transform(image_tensor)),\n",
    "        model_name, model, sim_metric\n",
    "    ) for transform in transforms_list]\n",
    "    \n",
    "    return sum(sims)/len(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict = {}\n",
    "sim_metric='ssim'\n",
    "# Calculate random pairs similarity\n",
    "temp_list = [calculate_similarity(load_image(f\"clean_images/coco_{i}.jpg\"), \n",
    "                                load_image(f\"clean_images/coco_{j}.jpg\"), \n",
    "                                model_name, model, sim_metric)\n",
    "             for i in range(1,10) for j in range(i+1,11)]\n",
    "sim_dict['random pairs'] = {'mean': round(torch.mean(torch.stack(temp_list)).item(), 3),\n",
    "                           'std': round(torch.std(torch.stack(temp_list)).item(), 3)}\n",
    "\n",
    "# # Calculate similarity between clean and their augmentations\n",
    "temp_list = [calculate_transformations_sim(load_image(f\"clean_images/coco_{i}.jpg\"),\n",
    "                                         model_name, model, sim_metric)\n",
    "             for i in range(1,11)]\n",
    "sim_dict['augmentations'] = {'mean': round(torch.mean(torch.stack(temp_list)).item(), 3),\n",
    "                            'std': round(torch.std(torch.stack(temp_list)).item(), 3)}\n",
    "\n",
    "# Calculate similarity between clean and perturbed images\n",
    "for key in file_dict:\n",
    "    temp_list = [calculate_similarity(load_image(f\"{file}clean_prompt.bmp\"),\n",
    "                                    load_image(f\"{file}bad_prompt.bmp\"),\n",
    "                                    model_name, model, sim_metric)\n",
    "                 for file in file_dict[key]]\n",
    "    sim_dict[key] = {'mean': round(torch.mean(torch.stack(temp_list)).item(), 3),\n",
    "                     'std': round(torch.std(torch.stack(temp_list)).item(), 3)}\n",
    "print(sim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calc_stats(means, stds, n=15):\n",
    "    mean_avg = np.mean(means)\n",
    "    S_within = np.sum((n-1) * stds**2)\n",
    "    S_between = np.sum(n * (means - mean_avg)**2)\n",
    "    var_avg = (S_within + S_between) / (len(means)*n - 1)\n",
    "    return {'mean': mean_avg, 'std': np.sqrt(var_avg)}\n",
    "\n",
    "result = {task: calc_stats(\n",
    "    np.array([sim_dict[k]['mean'] for k in keys]),\n",
    "    np.array([sim_dict[k]['std'] for k in keys])\n",
    ") for task, keys in categories.items()}\n",
    "\n",
    "overall = calc_stats(\n",
    "    np.array([r['mean'] for r in result.values()]),\n",
    "    np.array([r['std'] for r in result.values()])\n",
    ")\n",
    "\n",
    "for task in result:\n",
    "    print(f\"{task}: mean = {result[task]['mean']:.3f}, std = {result[task]['std']:.3f}\")\n",
    "print(f\"\\nOverall: mean = {overall['mean']:.3f}, std = {overall['std']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigpt4",
   "language": "python",
   "name": "minigpt4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
