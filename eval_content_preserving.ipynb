{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"output/model\"\n",
    "constraints = {\n",
    "    \"Baseline\": \"baseline_1\",\n",
    "    \"Positive\": \"Sentiment/Positive/constrained_eps_32_batch_8\",\n",
    "    \"Negative\": \"Sentiment/Negative/constrained_eps_32_batch_8\",\n",
    "    \"Neutral\": \"Sentiment/Neutral/constrained_eps_32_batch_8\",\n",
    "    \"Formal\": \"Formality/Formal/constrained_eps_32_batch_8\",\n",
    "    \"Informal\": \"Formality/Informal/constrained_eps_32_batch_8\",\n",
    "    \"English\": \"Language/English/constrained_eps_32_batch_8\",\n",
    "    \"French\": \"Language/French/constrained_eps_32_batch_8\",\n",
    "    \"Spanish\": \"Language/Spanish/constrained_eps_32_batch_8\",\n",
    "    \"Left\": \"Politics/Left/constrained_eps_32_batch_8\",\n",
    "    \"Right\": \"Politics/Right/constrained_eps_32_batch_8\",\n",
    "    \"Spam\": \"Attack/Spam/constrained_eps_32_batch_8\",\n",
    "    \"Injection\": \"Attack/Injection/constrained_eps_32_batch_8\"\n",
    "}\n",
    "\n",
    "file_dict = {}\n",
    "for key, constraint in constraints.items():\n",
    "    # First add paths with format base_path/i/constraint/\n",
    "    paths = [f\"{base_path}/{i}/{constraint}/\" for i in range(0,5)]\n",
    "    # Then add paths with format base_path/coco_i/constraint/\n",
    "    paths.extend([f\"{base_path}/coco_{i}/{constraint}/\" for i in range(1,11)])\n",
    "    file_dict[key] = paths\n",
    "\n",
    "model_name=\"minigpt4\"\n",
    "if model_name == 'minigpt4':\n",
    "    for category, paths in file_dict.items():\n",
    "        file_dict[category] = [path.replace(\"model\", model_name) for path in paths]\n",
    "elif model_name == 'llava':\n",
    "    for category, paths in file_dict.items():\n",
    "        file_dict[category] = [path.replace(\"model\", model_name) for path in paths]\n",
    "elif model_name == 'blip':\n",
    "    for category, paths in file_dict.items():\n",
    "        file_dict[category] = [path.replace(\"model\", model_name) for path in paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Baseline': {'mean': 1.0, 'std': 0.0},\n",
       " 'Positive': {'mean': 1.0, 'std': 0.0},\n",
       " 'Negative': {'mean': 0.97, 'std': 0.09},\n",
       " 'Neutral': {'mean': 0.93, 'std': 0.25},\n",
       " 'Formal': {'mean': 1.0, 'std': 0.01},\n",
       " 'Informal': {'mean': 1.0, 'std': 0.0},\n",
       " 'English': {'mean': 0.96, 'std': 0.11},\n",
       " 'French': {'mean': 0.94, 'std': 0.21},\n",
       " 'Spanish': {'mean': 0.98, 'std': 0.04},\n",
       " 'Left': {'mean': 0.88, 'std': 0.31},\n",
       " 'Right': {'mean': 1.0, 'std': 0.01},\n",
       " 'Spam': {'mean': 0.87, 'std': 0.31},\n",
       " 'Injection': {'mean': 0.98, 'std': 0.06},\n",
       " 'sentiment': {'mean': 0.9666666666666667, 'std': 0.153405779986718},\n",
       " 'language': {'mean': 0.96, 'std': 0.1388044187577134},\n",
       " 'formality': {'mean': 1.0, 'std': 0.007071067811865475},\n",
       " 'politics': {'mean': 0.94, 'std': 0.2193171219946131},\n",
       " 'Attack': {'mean': 0.925, 'std': 0.22327113561766107}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def get_classification_result(file_path):\n",
    "    # Initialize an empty list to store the extracted outputs\n",
    "    total_length=0\n",
    "    output_list = []\n",
    "    # Open the JSON file and read it line by line\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parse the JSON line\n",
    "            json_line = json.loads(line)\n",
    "\n",
    "            # Check if 'output' key exists in the JSON object\n",
    "            if 'output' in json_line:\n",
    "                # Add the value of 'output' key to the outputs list\n",
    "                output_list.append(json_line['output'])\n",
    "    count=0\n",
    "    #iterate through the list of instructions and print the sentiment\n",
    "    for o in output_list:\n",
    "        if \"yes\" in o.lower() or \"oui\" in o.lower():\n",
    "            count+=1\n",
    "            total_length+=1\n",
    "        elif \"no\" in o.lower():\n",
    "            total_length+=1\n",
    "        else:\n",
    "            count+=1\n",
    "            total_length+=1\n",
    "    return count/total_length\n",
    "\n",
    "\n",
    "classification_dict={}\n",
    "#iterate through the dictionary and calculate the result\n",
    "for key in file_dict:\n",
    "    result_count = []\n",
    "    for file in file_dict[key]:\n",
    "        result=get_classification_result(file+\"content_classification_result.jsonl\")\n",
    "        result_count.append(result)\n",
    "    import numpy as np\n",
    "    mean_value = sum(result_count)/len(result_count)\n",
    "    std_dev = np.std(result_count)\n",
    "    classification_dict[key] = {\"mean\": round(mean_value, 2), \"std\": round(std_dev, 2)}\n",
    "\n",
    "# Calculate means\n",
    "classification_dict[\"sentiment\"] = {\n",
    "    \"mean\": (classification_dict[\"Positive\"][\"mean\"] + classification_dict[\"Negative\"][\"mean\"] + classification_dict[\"Neutral\"][\"mean\"])/3,\n",
    "    \"std\": np.sqrt((classification_dict[\"Positive\"][\"std\"]**2 + classification_dict[\"Negative\"][\"std\"]**2 + classification_dict[\"Neutral\"][\"std\"]**2)/3)\n",
    "}\n",
    "classification_dict[\"language\"] = {\n",
    "    \"mean\": (classification_dict[\"English\"][\"mean\"] + classification_dict[\"French\"][\"mean\"] + classification_dict[\"Spanish\"][\"mean\"])/3,\n",
    "    \"std\": np.sqrt((classification_dict[\"English\"][\"std\"]**2 + classification_dict[\"French\"][\"std\"]**2 + classification_dict[\"Spanish\"][\"std\"]**2)/3)\n",
    "}\n",
    "classification_dict[\"formality\"] = {\n",
    "    \"mean\": (classification_dict[\"Formal\"][\"mean\"] + classification_dict[\"Informal\"][\"mean\"])/2,\n",
    "    \"std\": np.sqrt((classification_dict[\"Formal\"][\"std\"]**2 + classification_dict[\"Informal\"][\"std\"]**2)/2)\n",
    "}\n",
    "classification_dict[\"politics\"] = {\n",
    "    \"mean\": (classification_dict[\"Left\"][\"mean\"] + classification_dict[\"Right\"][\"mean\"])/2,\n",
    "    \"std\": np.sqrt((classification_dict[\"Left\"][\"std\"]**2 + classification_dict[\"Right\"][\"std\"]**2)/2)\n",
    "}\n",
    "classification_dict[\"Attack\"] = {\n",
    "    \"mean\": (classification_dict[\"Spam\"][\"mean\"] + classification_dict[\"Injection\"][\"mean\"])/2,\n",
    "    \"std\": np.sqrt((classification_dict[\"Spam\"][\"std\"]**2 + classification_dict[\"Injection\"][\"std\"]**2)/2)\n",
    "}\n",
    "\n",
    "classification_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity between oringinal and perturbed image\n",
    "## LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Initializing Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "You are using a model of type llava to instantiate a model of type llava_llama_2. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model =  ./ckpts/llava_llama_2_13b_chat_freeze\n",
      "llava_llama_2_13b_chat_freeze\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tz362/anaconda3/envs/minigpt4_new/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f286d6dbc3475e9a39a0ecbe88ece0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tz362/anaconda3/envs/minigpt4_new/lib/python3.9/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Initialization Finished]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"./ckpts/llava_llama_2_13b_chat_freeze\")\n",
    "    parser.add_argument(\"--gpu_id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--model_base\", type=str, default=None)\n",
    "    args = parser.parse_args(args=['--gpu_id', '0'])\n",
    "    return args\n",
    "\n",
    "# ========================================\n",
    "#             Model Initialization\n",
    "# ========================================\n",
    "print('>>> Initializing Models')\n",
    "from llava_llama_2.utils import get_model\n",
    "args = parse_args()\n",
    "print('model = ', args.model_path)\n",
    "tokenizer, model, image_processor, model_name = get_model(args)\n",
    "model.eval()\n",
    "print('[Initialization Finished]\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minigpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tz362/anaconda3/envs/minigpt4_new/lib/python3.9/site-packages/timm/models/hub.py:4: FutureWarning: Importing from timm.models.hub is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/tz362/anaconda3/envs/minigpt4_new/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/tz362/anaconda3/envs/minigpt4_new/lib/python3.9/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/tz362/anaconda3/envs/minigpt4_new/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Initializing Models\n",
      "Loading VIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/shmatikov/tingwei/Desktop/USENIX_25/Soft_prompt_go_hard/minigpt4/models/eva_vit.py:433: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(cached_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VIT Done\n",
      "Loading Q-Former\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 413M/413M [00:04<00:00, 99.7MB/s] \n",
      "/share/shmatikov/tingwei/Desktop/USENIX_25/Soft_prompt_go_hard/minigpt4/models/blip2.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(cached_file, map_location=\"cpu\")\n",
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Q-Former Done\n",
      "Loading LLAMA\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219c0037e7e043979f2bf03712c0248e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tz362/anaconda3/envs/minigpt4_new/lib/python3.9/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAMA Done\n",
      "Load BLIP2-LLM Checkpoint: ckpts/pretrained_minigpt4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/shmatikov/tingwei/Desktop/USENIX_25/Soft_prompt_go_hard/minigpt4/models/mini_gpt4.py:274: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization Finished\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "# import gradio as gr\n",
    "from PIL import Image\n",
    "\n",
    "from minigpt4.common.config import Config\n",
    "from minigpt4.common.dist_utils import get_rank\n",
    "from minigpt4.common.registry import registry\n",
    "\n",
    "# imports modules for registration\n",
    "from minigpt4.datasets.builders import *\n",
    "from minigpt4.models import *\n",
    "from minigpt4.processors import *\n",
    "from minigpt4.runners import *\n",
    "from minigpt4.tasks import *\n",
    "\n",
    "from minigpt_utils import prompt_wrapper, generator\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "\n",
    "    parser.add_argument(\"--cfg-path\", default=\"eval_configs/minigpt4_eval.yaml\", help=\"path to configuration file.\")\n",
    "    parser.add_argument(\"--gpu-id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--mode\", type=str, default='VisualChatBot',\n",
    "                        choices=[ \"TextOnly\", \"VisualChatBot\" ],\n",
    "                        help=\"Inference Mode: TextOnly: Text model only (Vicuna) \\n VisualChatBot: Vision model + Text model (MiniGPT4) \")\n",
    "    parser.add_argument(\"--image_file\", type=str, default='./image.bmp',\n",
    "                        help=\"Image file\")\n",
    "    parser.add_argument(\"--output_file\", type=str, default='./result.jsonl',\n",
    "                        help=\"Output file.\")\n",
    "    parser.add_argument(\n",
    "        \"--options\",\n",
    "        nargs=\"+\",\n",
    "        help=\"override some settings in the used config, the key-value pair \"\n",
    "        \"in xxx=yyy format will be merged into config file (deprecate), \"\n",
    "        \"change to --cfg-options instead.\",\n",
    "    )\n",
    "    args = parser.parse_args(args=['--cfg-path', 'eval_configs/minigpt4_eval.yaml', \n",
    "                               '--gpu-id', '0', '--mode', 'VisualChatBot', '--image_file', 'clean_images/0.png', '--output_file', './result.jsonl'])\n",
    "    # args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def setup_seeds(config):\n",
    "    seed = config.run_cfg.seed + get_rank()\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "\n",
    "# ========================================\n",
    "#             Model Initialization\n",
    "# ========================================\n",
    "\n",
    "print('>>> Initializing Models')\n",
    "args = parse_args()\n",
    "cfg = Config(args)\n",
    "model_config = cfg.model_cfg\n",
    "model_config.device_8bit = args.gpu_id\n",
    "model_cls = registry.get_model_class(model_config.arch)\n",
    "model = model_cls.from_config(model_config).to('cuda:{}'.format(args.gpu_id))\n",
    "vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\n",
    "vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "print('Initialization Finished')\n",
    "my_generator = generator.Generator(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Initializing Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tz362/anaconda3/envs/blip/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tz362/anaconda3/envs/blip/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/share/shmatikov/tingwei/Desktop/USENIX_25/Soft_prompt_go_hard/lavis/models/eva_vit.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(cached_file, map_location=\"cpu\")\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.14s/it]\n",
      "100%|██████████| 2.12G/2.12G [00:16<00:00, 138MB/s] \n",
      "/share/shmatikov/tingwei/Desktop/USENIX_25/Soft_prompt_go_hard/lavis/models/blip2_models/blip2.py:90: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(cached_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSource code of the model in:\\n    ./lavis/models/blip2_models/blip2_vicuna_instruct.py\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "    parser.add_argument(\"--gpu-id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"instruction_data/0/Sentiment/dataset.csv\")\n",
    "    parser.add_argument(\"--image_file\", type=str, default='./image.bmp',\n",
    "                        help=\"Image file\")\n",
    "    parser.add_argument(\"--output_file\", type=str, default='./result.jsonl',\n",
    "                        help=\"Output file.\")\n",
    "    parser.add_argument(\"--instruction\", type=str, default=None,\n",
    "                    choices=[ \"positive\", \"negative\", \"neutral\", \"irony\", \"non_irony\", \"formal\", \"informal\", \"french\", \"english\", \"spanish\", \"left\",\"right\",\"inference_content_evaluation\",\"injection\",\"spam\"],\n",
    "                        help=\"Instruction to be used for the attack.\")\n",
    "    parser.add_argument(\"--image_index\", type=int, default=0)\n",
    "    # args = parser.parse_args()\n",
    "    args = parser.parse_args(args=['--data_path', 'instruction_data/0/Attack/dataset.csv', \n",
    "                                '--image_file', 'clean_images/0.png', '--output_file', './result.jsonl'])\n",
    "    return args\n",
    "\n",
    "\n",
    "# ========================================\n",
    "#             Model Initialization\n",
    "# ========================================\n",
    "\n",
    "print('>>> Initializing Models')\n",
    "\n",
    "from lavis.models import load_model_and_preprocess\n",
    "\n",
    "args = parse_args()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# remember to modify the parameter llm_model in ./lavis/configs/models/blip2/blip2_instruct_vicuna13b.yaml to the path that store the vicuna weights\n",
    "model, vis_processor, _ = load_model_and_preprocess(\n",
    "        name='blip2_vicuna_instruct',\n",
    "        model_type='vicuna13b',\n",
    "        is_eval=True,\n",
    "        device=device,\n",
    "    )\n",
    "model.eval()\n",
    "\"\"\"\n",
    "Source code of the model in:\n",
    "    ./lavis/models/blip2_models/blip2_vicuna_instruct.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pytorch_msssim import ssim\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def denormalize(images):\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()\n",
    "    images = images * std[None, :, None, None]\n",
    "    images = images + mean[None, :, None, None]\n",
    "    return images\n",
    "\n",
    "def load_image(image_path):\n",
    "    return Image.open(image_path).convert('RGB')\n",
    "\n",
    "def calculate_similarity(image1, image2, metric, model, model_name):\n",
    "    # Prepare images based on model type\n",
    "    if not isinstance(image1, torch.Tensor):\n",
    "        if model_name == 'llava':\n",
    "            image1 = image_processor.preprocess(image1, return_tensors='pt')['pixel_values'].cuda()\n",
    "            image2 = image_processor.preprocess(image2, return_tensors='pt')['pixel_values'].cuda()\n",
    "        elif model_name == 'minigpt4':\n",
    "            image1 = vis_processor(image1).unsqueeze(0).to(model.device)\n",
    "            image2 = vis_processor(image2).unsqueeze(0).to(model.device)\n",
    "        elif model_name == 'blip':\n",
    "            image1 = vis_processor[\"eval\"](image1).unsqueeze(0).to(device)\n",
    "            image2 = vis_processor[\"eval\"](image2).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Calculate similarity based on metric\n",
    "    if metric == 'ssim':\n",
    "        return ssim(image1, image2, data_range=1, size_average=True)\n",
    "    \n",
    "    # For embedding-based metrics\n",
    "    cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    \n",
    "    if metric == 'llava':\n",
    "        emb_image1 = model.encode_images(image1.half())\n",
    "        emb_image2 = model.encode_images(image2.half())\n",
    "    elif metric == 'minigpt4':\n",
    "        image1, image2 = [image1], [image2]\n",
    "        prompt1 = prompt_wrapper.Prompt(model=model, img_prompts=[image1])\n",
    "        prompt2 = prompt_wrapper.Prompt(model=model, img_prompts=[image2])\n",
    "        emb_image1 = prompt1.img_embs[0][0]\n",
    "        emb_image2 = prompt2.img_embs[0][0]\n",
    "    elif metric == 'blip':\n",
    "        with model.maybe_autocast():\n",
    "            emb_image1 = model.ln_vision(model.visual_encoder(image1))\n",
    "            emb_image2 = model.ln_vision(model.visual_encoder(image2))\n",
    "    \n",
    "    return cos(emb_image1.view(-1).to(torch.float32), emb_image2.view(-1).to(torch.float32))\n",
    "\n",
    "def calculate_transformations_sim(image, metric, model, model_name):\n",
    "    transformations = [\n",
    "        transforms.GaussianBlur(kernel_size=9, sigma=(1.0, 5.0)),\n",
    "        transforms.RandomAffine(degrees=45),\n",
    "        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.2),\n",
    "        transforms.RandomHorizontalFlip(p=1),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5, p=1)\n",
    "    ]\n",
    "    # Prepare image tensor based on model\n",
    "    if model_name == 'llava':\n",
    "        image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].cuda()\n",
    "    elif model_name == 'minigpt4':\n",
    "        image_tensor = vis_processor(image).unsqueeze(0).cuda()\n",
    "    elif model_name == 'blip':\n",
    "        image_tensor = vis_processor[\"eval\"](image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Calculate similarity for each transformation\n",
    "    similarities = []\n",
    "    for transform in transformations:\n",
    "        transformed_image = transform(image_tensor)\n",
    "        denorm_img = denormalize(image_tensor)\n",
    "        denorm_transformed = denormalize(transformed_image)\n",
    "        sim = calculate_similarity(denorm_img, denorm_transformed, metric, model, model_name)\n",
    "        similarities.append(sim)\n",
    "    \n",
    "    return sum(similarities) / len(similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/shmatikov/tingwei/Desktop/USENIX_25/Soft_prompt_go_hard/minigpt4/models/blip2.py:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast(dtype=dtype)\n"
     ]
    }
   ],
   "source": [
    "sim_dict={}\n",
    "# sim_metric='minigpt4'\n",
    "sim_metric='minigpt4'\n",
    "model_name='minigpt4'\n",
    "# sim_metric='ssim'\n",
    "temp_list=[]\n",
    "for i in range(1,10):\n",
    "    image_file1=\"clean_images/coco_\" + str(i) + \".jpg\"\n",
    "    image1 = load_image(image_file1)\n",
    "    for j in range(i+1,11):\n",
    "        image_file2=\"clean_images/coco_\" + str(j) + \".jpg\"\n",
    "        image2 = load_image(image_file2)\n",
    "        sim =calculate_similarity(image1, image2, sim_metric, model, model_name)\n",
    "        temp_list.append(sim)\n",
    "sim_dict['random pairs'] = {'mean': round(torch.mean(torch.stack(temp_list)).item(), 3),\n",
    "                           'std': round(torch.std(torch.stack(temp_list)).item(), 3)}\n",
    "for key in file_dict:\n",
    "    if key==\"Baseline\":\n",
    "        continue\n",
    "    temp_list=[]\n",
    "    for file in file_dict[key]:\n",
    "        image_file1=file+\"clean_prompt.bmp\"\n",
    "        image1 = load_image(image_file1)\n",
    "        image_file2=file+\"bad_prompt.bmp\"\n",
    "        image2 = load_image(image_file2)\n",
    "        sim =calculate_similarity(image1, image2, sim_metric, model, model_name)\n",
    "        temp_list.append(sim)\n",
    "    #add key and the mean of the cosine similarity to a dictionary\n",
    "    sim_dict[key]= {'mean': round(torch.mean(torch.stack(temp_list)).item(), 3),\n",
    "                           'std': round(torch.std(torch.stack(temp_list)).item(), 3)}\n",
    "\n",
    "# Call the function with the dummy_loader and the list of transformations\n",
    "temp_list=[]\n",
    "for i in range(1,11):\n",
    "    image_file=\"clean_images/coco_\" + str(i) + \".jpg\"\n",
    "    image = load_image(image_file)\n",
    "    temp_list.append(calculate_transformations_sim(image, sim_metric, model, model_name))\n",
    "sim_dict['augmentations']= {'mean': round(torch.mean(torch.stack(temp_list)).item(), 3),\n",
    "                           'std': round(torch.std(torch.stack(temp_list)).item(), 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment: mean = 0.557, std = 0.121\n",
      "language: mean = 0.584, std = 0.119\n",
      "formality: mean = 0.600, std = 0.096\n",
      "politics: mean = 0.568, std = 0.136\n",
      "attack: mean = 0.592, std = 0.136\n",
      "\n",
      "Overall: mean = 0.580, std = 0.120\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 15  # sample size per sub row\n",
    "\n",
    "# Define groups\n",
    "groups = {\n",
    "    'sentiment': ['Positive', 'Negative', 'Neutral'],\n",
    "    'language': ['English', 'French', 'Spanish'],\n",
    "    'formality': ['Formal', 'Informal'],\n",
    "    'politics': ['Left', 'Right'],\n",
    "    'attack': ['Spam', 'Injection']\n",
    "}\n",
    "\n",
    "result = {}\n",
    "\n",
    "for task, keys in groups.items():\n",
    "    means = np.array([sim_dict[k]['mean'] for k in keys])\n",
    "    stds = np.array([sim_dict[k]['std'] for k in keys])\n",
    "    \n",
    "    mean_avg = np.mean(means)\n",
    "    \n",
    "    S_within = np.sum((n - 1) * stds**2)\n",
    "    S_between = np.sum(n * (means - mean_avg)**2)\n",
    "    S_total = S_within + S_between\n",
    "    var_avg = S_total / (len(keys) * n - 1)\n",
    "    std_avg = np.sqrt(var_avg)\n",
    "    \n",
    "    result[task] = {'mean': mean_avg, 'std': std_avg}\n",
    "\n",
    "# Compute overall across all 5 tasks\n",
    "all_means = np.array([result[task]['mean'] for task in result])\n",
    "all_stds = np.array([result[task]['std'] for task in result])\n",
    "\n",
    "mean_overall = np.mean(all_means)\n",
    "\n",
    "S_within_overall = np.sum((n - 1) * all_stds**2)\n",
    "S_between_overall = np.sum(n * (all_means - mean_overall)**2)\n",
    "S_total_overall = S_within_overall + S_between_overall\n",
    "var_overall = S_total_overall / (len(all_means) * n - 1)\n",
    "std_overall = np.sqrt(var_overall)\n",
    "\n",
    "# Print results\n",
    "for task in result:\n",
    "    print(f\"{task}: mean = {result[task]['mean']:.3f}, std = {result[task]['std']:.3f}\")\n",
    "print(f\"\\nOverall: mean = {mean_overall:.3f}, std = {std_overall:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigpt4",
   "language": "python",
   "name": "minigpt4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
