{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize a empty dictoinary to store the sentiment and the count\n",
    "file_dict = {\"Baseline\":[\"output/model/0/baseline_1/\",\n",
    "                           \"output/model/1/baseline_1/\",\n",
    "                           \"output/model/2/baseline_1/\",\n",
    "                           \"output/model/3/baseline_1/\",\n",
    "                           \"output/model/4/baseline_1/\"],\n",
    "             \"Positive\":[\"output/model/0/Sentiment/Positive/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Sentiment/Positive/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Sentiment/Positive/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Sentiment/Positive/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Sentiment/Positive/constrained_eps_32_batch_8/\"],\n",
    "                \"Negative\":[\"output/model/0/Sentiment/Negative/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Sentiment/Negative/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Sentiment/Negative/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Sentiment/Negative/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Sentiment/Negative/constrained_eps_32_batch_8/\"],\n",
    "                \"Neutral\":[\"output/model/0/Sentiment/Neutral/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Sentiment/Neutral/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Sentiment/Neutral/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Sentiment/Neutral/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Sentiment/Neutral/constrained_eps_32_batch_8/\"],\n",
    "                \"Formal\":[\"output/model/0/Formality/Formal/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Formality/Formal/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Formality/Formal/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Formality/Formal/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Formality/Formal/constrained_eps_32_batch_8/\"],\n",
    "                \"Informal\":[\"output/model/0/Formality/Informal/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Formality/Informal/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Formality/Informal/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Formality/Informal/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Formality/Informal/constrained_eps_32_batch_8/\"],\n",
    "                \"English\":[\"output/model/0/Language/English/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Language/English/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Language/English/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Language/English/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Language/English/constrained_eps_32_batch_8/\"],\n",
    "                \"French\":[\"output/model/0/Language/French/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Language/French/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Language/French/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Language/French/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Language/French/constrained_eps_32_batch_8/\"],    \n",
    "                \"Spanish\":[\"output/model/0/Language/Spanish/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Language/Spanish/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Language/Spanish/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Language/Spanish/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Language/Spanish/constrained_eps_32_batch_8/\"],  \n",
    "                \"Left\":[\"output/model/0/Politics/Left/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Politics/Left/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Politics/Left/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Politics/Left/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Politics/Left/constrained_eps_32_batch_8/\"],  \n",
    "                \"Right\":[\"output/model/0/Politics/Right/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Politics/Right/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Politics/Right/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Politics/Right/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Politics/Right/constrained_eps_32_batch_8/\"],                            \n",
    "              }\n",
    "\n",
    "\n",
    "modal_name=\"llava\"\n",
    "if modal_name == 'minigpt4':\n",
    "    for category, paths in file_dict.items():\n",
    "        file_dict[category] = [path.replace(\"model\", modal_name) for path in paths]\n",
    "elif modal_name == 'llava':\n",
    "    for category, paths in file_dict.items():\n",
    "        file_dict[category] = [path.replace(\"model\", modal_name) for path in paths]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_classification_result(file_path):\n",
    "    # Initialize an empty list to store the extracted outputs\n",
    "    total_length=0\n",
    "    output_list = []\n",
    "    # Open the JSON file and read it line by line\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parse the JSON line\n",
    "            json_line = json.loads(line)\n",
    "\n",
    "            # Check if 'output' key exists in the JSON object\n",
    "            if 'output' in json_line:\n",
    "                # Add the value of 'output' key to the outputs list\n",
    "                output_list.append(json_line['output'])\n",
    "    count=0\n",
    "    #iterate through the list of instructions and print the sentiment\n",
    "    for o in output_list:\n",
    "        if \"yes\" in o.lower():\n",
    "            count+=1\n",
    "            total_length+=1\n",
    "        elif \"no\" in o.lower():\n",
    "            total_length+=1\n",
    "    # print(file_path,count)\n",
    "    return count/total_length\n",
    "\n",
    "\n",
    "classification_dict={}\n",
    "#iterate through the dictionary and calculate the result\n",
    "for key in file_dict:\n",
    "    result_count = []\n",
    "    for file in file_dict[key]:\n",
    "        result=get_classification_result(file+\"content_classification_result.jsonl\")\n",
    "        result_count.append(result)\n",
    "    classification_dict[key]=round(sum(result_count)/len(result_count),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Baseline': 0.43,\n",
       " 'Positive': 0.69,\n",
       " 'Negative': 0.43,\n",
       " 'Neutral': 0.53,\n",
       " 'Formal': 0.47,\n",
       " 'Informal': 0.46,\n",
       " 'English': 0.65,\n",
       " 'French': 0.35,\n",
       " 'Spanish': 0.12,\n",
       " 'Left': 0.53,\n",
       " 'Right': 0.63,\n",
       " 'sentiment': 0.5499999999999999,\n",
       " 'formality': 0.46499999999999997,\n",
       " 'language': 0.37333333333333335,\n",
       " 'politics': 0.5800000000000001}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_dict[\"sentiment\"]=(classification_dict[\"Positive\"]+ classification_dict[\"Negative\"]+ classification_dict[\"Neutral\"])/3\n",
    "classification_dict[\"formality\"]=(classification_dict[\"Formal\"]+ classification_dict[\"Informal\"])/2\n",
    "classification_dict[\"language\"]=(classification_dict[\"English\"]+ classification_dict[\"French\"]+ classification_dict[\"Spanish\"])/3\n",
    "classification_dict[\"politics\"]=(classification_dict[\"Left\"]+ classification_dict[\"Right\"])/2\n",
    "classification_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_dict\n",
    "#save the dictionary to a json file\n",
    "import json\n",
    "with open('output/aggregated_result/content_perservation/classification/minigpt4_binary_32.json', 'w') as fp:\n",
    "    json.dump(classification_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity between oringinal and perturbed image\n",
    "## LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Initializing Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "You are using a model of type llava to instantiate a model of type llava_llama_2. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model =  ./ckpts/llava_llama_2_13b_chat_freeze\n",
      "llava_llama_2_13b_chat_freeze\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e7f481e1b148f096b827916fbc4ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Initialization Finished]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"./ckpts/llava_llama_2_13b_chat_freeze\")\n",
    "    parser.add_argument(\"--gpu_id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--model_base\", type=str, default=None)\n",
    "    args = parser.parse_args(args=['--gpu_id', '0'])\n",
    "    return args\n",
    "\n",
    "# ========================================\n",
    "#             Model Initialization\n",
    "# ========================================\n",
    "print('>>> Initializing Models')\n",
    "from llava_llama_2.utils import get_model\n",
    "args = parse_args()\n",
    "print('model = ', args.model_path)\n",
    "tokenizer, model, image_processor, model_name = get_model(args)\n",
    "model.eval()\n",
    "print('[Initialization Finished]\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minigpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Initializing Models\n",
      "Loading VIT\n",
      "Loading VIT Done\n",
      "Loading Q-Former\n",
      "Loading Q-Former Done\n",
      "Loading LLAMA\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab79fa0c599c4c87b72f2f9a4b690628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAMA Done\n",
      "Load BLIP2-LLM Checkpoint: ckpts/pretrained_minigpt4.pth\n",
      "Initialization Finished\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "# import gradio as gr\n",
    "from PIL import Image\n",
    "\n",
    "from minigpt4.common.config import Config\n",
    "from minigpt4.common.dist_utils import get_rank\n",
    "from minigpt4.common.registry import registry\n",
    "\n",
    "# imports modules for registration\n",
    "from minigpt4.datasets.builders import *\n",
    "from minigpt4.models import *\n",
    "from minigpt4.processors import *\n",
    "from minigpt4.runners import *\n",
    "from minigpt4.tasks import *\n",
    "\n",
    "from minigpt_utils import prompt_wrapper, generator\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "\n",
    "    parser.add_argument(\"--cfg-path\", default=\"eval_configs/minigpt4_eval.yaml\", help=\"path to configuration file.\")\n",
    "    parser.add_argument(\"--gpu-id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--mode\", type=str, default='VisualChatBot',\n",
    "                        choices=[ \"TextOnly\", \"VisualChatBot\" ],\n",
    "                        help=\"Inference Mode: TextOnly: Text model only (Vicuna) \\n VisualChatBot: Vision model + Text model (MiniGPT4) \")\n",
    "    parser.add_argument(\"--image_file\", type=str, default='./image.bmp',\n",
    "                        help=\"Image file\")\n",
    "    parser.add_argument(\"--output_file\", type=str, default='./result.jsonl',\n",
    "                        help=\"Output file.\")\n",
    "    parser.add_argument(\n",
    "        \"--options\",\n",
    "        nargs=\"+\",\n",
    "        help=\"override some settings in the used config, the key-value pair \"\n",
    "        \"in xxx=yyy format will be merged into config file (deprecate), \"\n",
    "        \"change to --cfg-options instead.\",\n",
    "    )\n",
    "    args = parser.parse_args(args=['--cfg-path', 'eval_configs/minigpt4_eval.yaml', \n",
    "                               '--gpu-id', '0', '--mode', 'VisualChatBot', '--image_file', 'clean_images/0.png', '--output_file', './result.jsonl'])\n",
    "    # args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def setup_seeds(config):\n",
    "    seed = config.run_cfg.seed + get_rank()\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "\n",
    "# ========================================\n",
    "#             Model Initialization\n",
    "# ========================================\n",
    "\n",
    "print('>>> Initializing Models')\n",
    "args = parse_args()\n",
    "cfg = Config(args)\n",
    "model_config = cfg.model_cfg\n",
    "model_config.device_8bit = args.gpu_id\n",
    "model_cls = registry.get_model_class(model_config.arch)\n",
    "model = model_cls.from_config(model_config).to('cuda:{}'.format(args.gpu_id))\n",
    "vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\n",
    "vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "print('Initialization Finished')\n",
    "my_generator = generator.Generator(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pytorch_msssim import ssim\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def denormalize(images):\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()\n",
    "    images = images * std[None, :, None, None]\n",
    "    images = images + mean[None, :, None, None]\n",
    "    return images\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    return image\n",
    "\n",
    "def calculate_similarity(image1, image2, metric, model):\n",
    "    if metric=='llava':\n",
    "        cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "        if not isinstance(image1, torch.Tensor):\n",
    "            image1 = image_processor.preprocess(image1, return_tensors='pt')['pixel_values'].cuda()\n",
    "            image2 = image_processor.preprocess(image2, return_tensors='pt')['pixel_values'].cuda()\n",
    "        emb_image1 = model.encode_images(image1.half())\n",
    "        emb_image2 = model.encode_images(image2.half())\n",
    "        cos_sim = cos(emb_image1.view(-1).to(torch.float32), emb_image2.view(-1).to(torch.float32))\n",
    "        return cos_sim\n",
    "    elif metric=='minigpt4':\n",
    "        cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "        if not isinstance(image1, torch.Tensor):\n",
    "            image1 = [vis_processor(image1).unsqueeze(0).to(model.device)]\n",
    "            image2 = [vis_processor(image2).unsqueeze(0).to(model.device)]\n",
    "        else:\n",
    "            image1=[image1]\n",
    "            image2=[image2]\n",
    "        prompt1 = prompt_wrapper.Prompt(model=model, img_prompts=[image1])\n",
    "        emb_image1 = prompt1.img_embs[0][0]\n",
    "        prompt2 = prompt_wrapper.Prompt(model=model, img_prompts=[image2])\n",
    "        emb_image2 = prompt2.img_embs[0][0]\n",
    "        cos_sim = cos(emb_image1.view(-1).to(torch.float32), emb_image2.view(-1).to(torch.float32))\n",
    "        return cos_sim\n",
    "    elif metric=='ssim':\n",
    "        if not isinstance(image1, torch.Tensor):\n",
    "            image1 = image_processor.preprocess(image1, return_tensors='pt')['pixel_values'].cuda()\n",
    "            image2 = image_processor.preprocess(image2, return_tensors='pt')['pixel_values'].cuda()\n",
    "        ssim_val = ssim(image1, image2, data_range=1, size_average=True)  \n",
    "        return ssim_val\n",
    "\n",
    "def calculate_transformations_sim(image, metric, model, show_plots=True):\n",
    "\n",
    "    transformations = [\n",
    "        transforms.GaussianBlur(kernel_size=9, sigma=(1.0, 5.0)),\n",
    "        transforms.RandomAffine(degrees=45),  # Rotate by 45 degrees\n",
    "        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.2),\n",
    "        transforms.RandomHorizontalFlip(p=1),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5, p=1)\n",
    "    ]\n",
    "    \n",
    "    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].cuda()\n",
    "    if show_plots:\n",
    "        fig, axs = plt.subplots(1, len(transformations) + 1, figsize=(15, 10))  # +1 for the original image\n",
    "        axs[0].imshow(denormalize(image_tensor)[0].detach().cpu().permute(1, 2, 0))\n",
    "        axs[0].set_title('Original')\n",
    "        axs[0].axis('off')\n",
    "\n",
    "    sim_list = []\n",
    "    for idx, transform in enumerate(transformations):\n",
    "        transformed_image_tensor = transform(image_tensor)  # Apply transformation\n",
    "        if show_plots:\n",
    "            axs[idx + 1].imshow(denormalize(transformed_image_tensor)[0].detach().cpu().permute(1, 2, 0))\n",
    "            axs[idx + 1].set_title(transform.__class__.__name__)\n",
    "            axs[idx + 1].axis('off')\n",
    "        if metric=='ssim':\n",
    "            sim = calculate_similarity(denormalize(image_tensor), denormalize(transformed_image_tensor),model, metric)\n",
    "        else:\n",
    "            sim = calculate_similarity(denormalize(image_tensor), denormalize(transformed_image_tensor),model, metric)\n",
    "        sim_list.append(sim)\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    return sum(sim_list)/len(sim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m         temp_list\u001b[38;5;241m.\u001b[39mappend(sim)\n\u001b[1;32m     12\u001b[0m sim_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom pairs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mround\u001b[39m(torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mstack(temp_list))\u001b[38;5;241m.\u001b[39mitem(),\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfile_dict\u001b[49m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseline\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file_dict' is not defined"
     ]
    }
   ],
   "source": [
    "sim_dict={}\n",
    "sim_metric='llava'\n",
    "temp_list=[]\n",
    "for i in range(0,4):\n",
    "    image_file1=\"clean_images/\" + str(i) + \".png\"\n",
    "    image1 = load_image(image_file1)\n",
    "    for j in range(i+1,5):\n",
    "        image_file2=\"clean_images/\" + str(j) + \".png\"\n",
    "        image2 = load_image(image_file2)\n",
    "        sim =calculate_similarity(image1, image2, sim_metric, model)\n",
    "        temp_list.append(sim)\n",
    "sim_dict['random pairs']=round(torch.mean(torch.stack(temp_list)).item(),3)\n",
    "\n",
    "for key in file_dict:\n",
    "    if key==\"Baseline\":\n",
    "        continue\n",
    "    temp_list=[]\n",
    "    for file in file_dict[key]:\n",
    "        image_file1=file+\"clean_prompt.bmp\"\n",
    "        image1 = load_image(image_file1)\n",
    "        image_file2=file+\"bad_prompt.bmp\"\n",
    "        image2 = load_image(image_file2)\n",
    "        sim =calculate_similarity(image1, image2, sim_metric,model)\n",
    "        temp_list.append(sim)\n",
    "    #add key and the mean of the cosine similarity to a dictionary\n",
    "    sim_dict[key]=round(torch.mean(torch.stack(temp_list)).item(),3)\n",
    "\n",
    "# Call the function with the dummy_loader and the list of transformations\n",
    "temp_list=[]\n",
    "for i in range(0,5):\n",
    "    image_file=\"clean_images/\" + str(i) + \".png\"\n",
    "    image = load_image(image_file)\n",
    "    temp_list.append(calculate_transformations_sim(image, model, sim_metric, False))\n",
    "sim_dict['augmentations']=round(torch.mean(torch.stack(temp_list)).item(),3)\n",
    "\n",
    "print(sim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random pairs': 0.535,\n",
       " 'Positive': 0.641,\n",
       " 'Negative': 0.639,\n",
       " 'Neutral': 0.571,\n",
       " 'Formal': 0.682,\n",
       " 'Informal': 0.605,\n",
       " 'English': 0.612,\n",
       " 'French': 0.695,\n",
       " 'Spanish': 0.713,\n",
       " 'Left': 0.616,\n",
       " 'Right': 0.582,\n",
       " 'augmentations': 0.809,\n",
       " 'sentiment': 0.617,\n",
       " 'formality': 0.6435,\n",
       " 'language': 0.6733333333333333,\n",
       " 'politics': 0.599}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_dict[\"sentiment\"]=(sim_dict[\"Positive\"]+ sim_dict[\"Negative\"]+ sim_dict[\"Neutral\"])/3\n",
    "sim_dict[\"formality\"]=(sim_dict[\"Formal\"]+ sim_dict[\"Informal\"])/2\n",
    "sim_dict[\"language\"]=(sim_dict[\"English\"]+ sim_dict[\"French\"]+ sim_dict[\"Spanish\"])/3\n",
    "sim_dict[\"politics\"]=(sim_dict[\"Left\"]+ sim_dict[\"Right\"])/2\n",
    "sim_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random pairs': 0.535,\n",
       " 'Positive': 0.641,\n",
       " 'Negative': 0.639,\n",
       " 'Neutral': 0.571,\n",
       " 'Formal': 0.682,\n",
       " 'Informal': 0.605,\n",
       " 'English': 0.612,\n",
       " 'French': 0.695,\n",
       " 'Spanish': 0.713,\n",
       " 'Left': 0.616,\n",
       " 'Right': 0.582}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_dict\n",
    "#save the dictionary to a json file\n",
    "import json\n",
    "with open('output/aggregated_result/content_perservation/embedding/minigpt4_minigpt_result_32.json', 'w') as fp:\n",
    "    json.dump(sim_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llava': 0.311, 'minigpt4': 0.22}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jailbreak_dict = {\"llava\":[\"/home/tz362/Desktop/projects/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models/results_llava_llama_v2_constrained_32_image_2/\"],\n",
    "                \"minigpt4\":[\"/home/tz362/Desktop/projects/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models/visual_constrained_eps_32/\"],                            \n",
    "                }\n",
    "sim_metric='llava'\n",
    "sim_dict = {}\n",
    "for key in jailbreak_dict:\n",
    "    temp_list=[]\n",
    "    for file in jailbreak_dict[key]:\n",
    "        image_file1=\"/home/tz362/Desktop/projects/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models/adversarial_images/clean_2.png\"\n",
    "        image1 = load_image(image_file1)\n",
    "        image_file2=file+\"bad_prompt_temp_5000.bmp\"\n",
    "        image2 = load_image(image_file2)\n",
    "        sim =calculate_similarity(image1, image2, sim_metric,model)\n",
    "        temp_list.append(sim)\n",
    "    #add key and the mean of the cosine similarity to a dictionary\n",
    "    sim_dict[key]=round(torch.mean(torch.stack(temp_list)).item(),3)\n",
    "sim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigpt4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
