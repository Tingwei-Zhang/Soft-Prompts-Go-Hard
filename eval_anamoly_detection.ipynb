{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize a empty dictoinary to store the sentiment and the count\n",
    "file_dict = {\"Positive\":[\"output/model/0/Sentiment/Positive/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Sentiment/Positive/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Sentiment/Positive/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Sentiment/Positive/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Sentiment/Positive/constrained_eps_32_batch_8/\"],\n",
    "                \"Negative\":[\"output/model/0/Sentiment/Negative/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Sentiment/Negative/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Sentiment/Negative/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Sentiment/Negative/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Sentiment/Negative/constrained_eps_32_batch_8/\"],\n",
    "                \"Neutral\":[\"output/model/0/Sentiment/Neutral/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Sentiment/Neutral/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Sentiment/Neutral/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Sentiment/Neutral/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Sentiment/Neutral/constrained_eps_32_batch_8/\"],\n",
    "                \"Formal\":[\"output/model/0/Formality/Formal/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Formality/Formal/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Formality/Formal/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Formality/Formal/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Formality/Formal/constrained_eps_32_batch_8/\"],\n",
    "                \"Informal\":[\"output/model/0/Formality/Informal/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Formality/Informal/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Formality/Informal/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Formality/Informal/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Formality/Informal/constrained_eps_32_batch_8/\"],\n",
    "                \"English\":[\"output/model/0/Language/English/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Language/English/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Language/English/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Language/English/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Language/English/constrained_eps_32_batch_8/\"],\n",
    "                \"French\":[\"output/model/0/Language/French/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Language/French/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Language/French/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Language/French/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Language/French/constrained_eps_32_batch_8/\"],    \n",
    "                \"Spanish\":[\"output/model/0/Language/Spanish/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Language/Spanish/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Language/Spanish/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Language/Spanish/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Language/Spanish/constrained_eps_32_batch_8/\"],  \n",
    "                \"Left\":[\"output/model/0/Politics/Left/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Politics/Left/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Politics/Left/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Politics/Left/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Politics/Left/constrained_eps_32_batch_8/\"],  \n",
    "                \"Right\":[\"output/model/0/Politics/Right/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/1/Politics/Right/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/2/Politics/Right/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/3/Politics/Right/constrained_eps_32_batch_8/\",\n",
    "                           \"output/model/4/Politics/Right/constrained_eps_32_batch_8/\"],                            \n",
    "              }\n",
    "\n",
    "file_dict = {\"Positive\":[\"output/model/0/Sentiment/Positive/constrained_eps_32_batch_8_jpeg/\",\n",
    "                           \"output/model/1/Sentiment/Positive/constrained_eps_32_batch_8_jpeg/\",\n",
    "                           \"output/model/2/Sentiment/Positive/constrained_eps_32_batch_8_jpeg/\",\n",
    "                           \"output/model/3/Sentiment/Positive/constrained_eps_32_batch_8_jpeg/\",\n",
    "                           \"output/model/4/Sentiment/Positive/constrained_eps_32_batch_8_jpeg/\"],\n",
    "                \"Negative\":[\"output/model/0/Sentiment/Negative/constrained_eps_32_batch_8_jpeg/\",\n",
    "                           \"output/model/1/Sentiment/Negative/constrained_eps_32_batch_8_jpeg/\",\n",
    "                           \"output/model/2/Sentiment/Negative/constrained_eps_32_batch_8_jpeg/\",\n",
    "                           \"output/model/3/Sentiment/Negative/constrained_eps_32_batch_8_jpeg/\",\n",
    "                           \"output/model/4/Sentiment/Negative/constrained_eps_32_batch_8_jpeg/\"],\n",
    "                \"Neutral\":[\"output/model/0/Sentiment/Neutral/constrained_eps_32_batch_8_jpeg/\",\n",
    "                           \"output/model/1/Sentiment/Neutral/constrained_eps_32_batch_8_jpeg/\",\n",
    "                           \"output/model/2/Sentiment/Neutral/constrained_eps_32_batch_8_jpeg/\",\n",
    "                           \"output/model/3/Sentiment/Neutral/constrained_eps_32_batch_8_jpeg/\",\n",
    "                           \"output/model/4/Sentiment/Neutral/constrained_eps_32_batch_8_jpeg/\"],\n",
    "}\n",
    "\n",
    "modal_name=\"minigpt4\"\n",
    "if modal_name == 'minigpt4':\n",
    "    for category, paths in file_dict.items():\n",
    "        file_dict[category] = [path.replace(\"model\", modal_name) for path in paths]\n",
    "elif modal_name == 'llava':\n",
    "    for category, paths in file_dict.items():\n",
    "        file_dict[category] = [path.replace(\"model\", modal_name) for path in paths]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity between oringinal and perturbed image\n",
    "## LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Initializing Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "You are using a model of type llava to instantiate a model of type llava_llama_2. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model =  ./ckpts/llava_llama_2_13b_chat_freeze\n",
      "llava_llama_2_13b_chat_freeze\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tz362/anaconda3/envs/minigpt4/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eecc43ba08644bd99025ec0d4654de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Initialization Finished]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"./ckpts/llava_llama_2_13b_chat_freeze\")\n",
    "    parser.add_argument(\"--gpu_id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--model_base\", type=str, default=None)\n",
    "    args = parser.parse_args(args=['--gpu_id', '0'])\n",
    "    return args\n",
    "\n",
    "# ========================================\n",
    "#             Model Initialization\n",
    "# ========================================\n",
    "print('>>> Initializing Models')\n",
    "from llava_llama_2.utils import get_model\n",
    "args = parse_args()\n",
    "print('model = ', args.model_path)\n",
    "tokenizer, model, image_processor, model_name = get_model(args)\n",
    "model.eval()\n",
    "print('[Initialization Finished]\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minigpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Initializing Models\n",
      "Loading VIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tz362/anaconda3/envs/minigpt4/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VIT Done\n",
      "Loading Q-Former\n",
      "Loading Q-Former Done\n",
      "Loading LLAMA\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ab5493ab2e455cafc8c6f7943de167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAMA Done\n",
      "Load BLIP2-LLM Checkpoint: ckpts/pretrained_minigpt4.pth\n",
      "Initialization Finished\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "# import gradio as gr\n",
    "from PIL import Image\n",
    "\n",
    "from minigpt4.common.config import Config\n",
    "from minigpt4.common.dist_utils import get_rank\n",
    "from minigpt4.common.registry import registry\n",
    "\n",
    "# imports modules for registration\n",
    "from minigpt4.datasets.builders import *\n",
    "from minigpt4.models import *\n",
    "from minigpt4.processors import *\n",
    "from minigpt4.runners import *\n",
    "from minigpt4.tasks import *\n",
    "\n",
    "from minigpt_utils import prompt_wrapper, generator\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "\n",
    "    parser.add_argument(\"--cfg-path\", default=\"eval_configs/minigpt4_eval.yaml\", help=\"path to configuration file.\")\n",
    "    parser.add_argument(\"--gpu-id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--mode\", type=str, default='VisualChatBot',\n",
    "                        choices=[ \"TextOnly\", \"VisualChatBot\" ],\n",
    "                        help=\"Inference Mode: TextOnly: Text model only (Vicuna) \\n VisualChatBot: Vision model + Text model (MiniGPT4) \")\n",
    "    parser.add_argument(\"--image_file\", type=str, default='./image.bmp',\n",
    "                        help=\"Image file\")\n",
    "    parser.add_argument(\"--output_file\", type=str, default='./result.jsonl',\n",
    "                        help=\"Output file.\")\n",
    "    parser.add_argument(\n",
    "        \"--options\",\n",
    "        nargs=\"+\",\n",
    "        help=\"override some settings in the used config, the key-value pair \"\n",
    "        \"in xxx=yyy format will be merged into config file (deprecate), \"\n",
    "        \"change to --cfg-options instead.\",\n",
    "    )\n",
    "    args = parser.parse_args(args=['--cfg-path', 'eval_configs/minigpt4_eval.yaml', \n",
    "                               '--gpu-id', '0', '--mode', 'VisualChatBot', '--image_file', 'clean_images/0.png', '--output_file', './result.jsonl'])\n",
    "    # args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def setup_seeds(config):\n",
    "    seed = config.run_cfg.seed + get_rank()\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "\n",
    "# ========================================\n",
    "#             Model Initialization\n",
    "# ========================================\n",
    "\n",
    "print('>>> Initializing Models')\n",
    "args = parse_args()\n",
    "cfg = Config(args)\n",
    "model_config = cfg.model_cfg\n",
    "model_config.device_8bit = args.gpu_id\n",
    "model_cls = registry.get_model_class(model_config.arch)\n",
    "model = model_cls.from_config(model_config).to('cuda:{}'.format(args.gpu_id))\n",
    "vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\n",
    "vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "print('Initialization Finished')\n",
    "my_generator = generator.Generator(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pytorch_msssim import ssim\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def denormalize(images):\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()\n",
    "    images = images * std[None, :, None, None]\n",
    "    images = images + mean[None, :, None, None]\n",
    "    return images\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    return image\n",
    "\n",
    "def calculate_similarity(image1, image2, metric, model):\n",
    "    if metric=='llava':\n",
    "        cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "        if not isinstance(image1, torch.Tensor):\n",
    "            image1 = image_processor.preprocess(image1, return_tensors='pt')['pixel_values'].cuda()\n",
    "            image2 = image_processor.preprocess(image2, return_tensors='pt')['pixel_values'].cuda()\n",
    "        emb_image1 = model.encode_images(image1.half())\n",
    "        emb_image2 = model.encode_images(image2.half())\n",
    "        cos_sim = cos(emb_image1.view(-1).to(torch.float32), emb_image2.view(-1).to(torch.float32))\n",
    "        return cos_sim\n",
    "    elif metric=='minigpt4':\n",
    "        cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "        if not isinstance(image1, torch.Tensor):\n",
    "            image1 = [vis_processor(image1).unsqueeze(0).to(model.device)]\n",
    "            image2 = [vis_processor(image2).unsqueeze(0).to(model.device)]\n",
    "        else:\n",
    "            image1=[image1]\n",
    "            image2=[image2]\n",
    "        prompt1 = prompt_wrapper.Prompt(model=model, img_prompts=[image1])\n",
    "        emb_image1 = prompt1.img_embs[0][0]\n",
    "        prompt2 = prompt_wrapper.Prompt(model=model, img_prompts=[image2])\n",
    "        emb_image2 = prompt2.img_embs[0][0]\n",
    "        cos_sim = cos(emb_image1.view(-1).to(torch.float32), emb_image2.view(-1).to(torch.float32))\n",
    "        return cos_sim\n",
    "    elif metric=='ssim':\n",
    "        if not isinstance(image1, torch.Tensor):\n",
    "            image1 = image_processor.preprocess(image1, return_tensors='pt')['pixel_values'].cuda()\n",
    "            image2 = image_processor.preprocess(image2, return_tensors='pt')['pixel_values'].cuda()\n",
    "        ssim_val = ssim(image1, image2, data_range=1, size_average=True)  \n",
    "        return ssim_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function jpeg_transform at 0x7f6bfbf4c5e0>\n",
      "GaussianBlur(kernel_size=(9, 9), sigma=(1.0, 5.0))\n",
      "RandomAffine(degrees=[-45.0, 45.0])\n",
      "ColorJitter(brightness=(0.5, 1.5), contrast=(0.5, 1.5), saturation=(0.5, 1.5), hue=(-0.2, 0.2))\n",
      "RandomHorizontalFlip(p=1)\n",
      "RandomPerspective(p=0.5)\n",
      "{<function jpeg_transform at 0x7f6bfbf4c5e0>: {'mean': 0.836, 'std': 0.086}, GaussianBlur(kernel_size=(9, 9), sigma=(1.0, 5.0)): {'mean': 0.805, 'std': 0.082}, RandomAffine(degrees=[-45.0, 45.0]): {'mean': 0.775, 'std': 0.068}, ColorJitter(brightness=(0.5, 1.5), contrast=(0.5, 1.5), saturation=(0.5, 1.5), hue=(-0.2, 0.2)): {'mean': 0.814, 'std': 0.085}, RandomHorizontalFlip(p=1): {'mean': 0.846, 'std': 0.09}, RandomPerspective(p=0.5): {'mean': 0.875, 'std': 0.121}}\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import save_image\n",
    "sim_dict={}\n",
    "sim_metric='minigpt4'\n",
    "temp_list=[]\n",
    "\n",
    "def jpeg_transform(X):\n",
    "    image_filename = 'dummy.jpg'\n",
    "    save_image(torch.squeeze(denormalize(X)), image_filename)\n",
    "    image = load_image(image_filename)\n",
    "    if sim_metric == 'llava' or sim_metric == 'ssim':\n",
    "        jpeg_X = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].cuda()\n",
    "    else:\n",
    "        jpeg_X = vis_processor(image).unsqueeze(0).to(model.device)\n",
    "    jpeg_X=jpeg_X.clamp(0,1)\n",
    "    return jpeg_X.cpu()\n",
    "\n",
    "# Define a list of transformations to apply\n",
    "transformations = [\n",
    "    jpeg_transform,\n",
    "    transforms.GaussianBlur(kernel_size=9, sigma=(1.0, 5.0)),\n",
    "    transforms.RandomAffine(degrees=45),  # Rotate by 45 degrees\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.2),\n",
    "    transforms.RandomHorizontalFlip(p=1),\n",
    "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5)\n",
    "]\n",
    "\n",
    "\n",
    "for idx, transform in enumerate(transformations):\n",
    "    print(transform)\n",
    "    temp_list = []\n",
    "    for key in file_dict:\n",
    "        for file in file_dict[key]:\n",
    "            image_file1 = file + \"bad_prompt.bmp\"\n",
    "            image1 = load_image(image_file1)\n",
    "            if sim_metric == 'llava' or sim_metric == 'ssim':\n",
    "                image1_tensor = image_processor.preprocess(image1, return_tensors='pt')['pixel_values'].cuda()\n",
    "            else:\n",
    "                image1_tensor = vis_processor(image1).unsqueeze(0).to(model.device)\n",
    "            transformed_image_tensor = transform(image1_tensor).cuda()\n",
    "            sim = calculate_similarity(image1_tensor, transformed_image_tensor, sim_metric, model)\n",
    "            temp_list.append(sim)\n",
    "    mean_sim = torch.mean(torch.stack(temp_list)).item()\n",
    "    std_sim = torch.std(torch.stack(temp_list)).item()\n",
    "    sim_dict[transform] = {'mean': round(mean_sim, 3), 'std': round(std_sim, 3)}\n",
    "\n",
    "# Print the final dictionary with mean and standard deviation values\n",
    "print(sim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function jpeg_transform at 0x7fee43d48280>\n",
      "GaussianBlur(kernel_size=(9, 9), sigma=(1.0, 5.0))\n",
      "RandomAffine(degrees=[-45.0, 45.0])\n",
      "ColorJitter(brightness=(0.5, 1.5), contrast=(0.5, 1.5), saturation=(0.5, 1.5), hue=(-0.2, 0.2))\n",
      "RandomHorizontalFlip(p=1)\n",
      "RandomPerspective(p=0.5)\n",
      "{<function jpeg_transform at 0x7fee43d48280>: {'mean': 0.414, 'std': 0.068}, GaussianBlur(kernel_size=(9, 9), sigma=(1.0, 5.0)): {'mean': 0.52, 'std': 0.113}, RandomAffine(degrees=[-45.0, 45.0]): {'mean': 0.391, 'std': 0.14}, ColorJitter(brightness=(0.5, 1.5), contrast=(0.5, 1.5), saturation=(0.5, 1.5), hue=(-0.2, 0.2)): {'mean': 0.362, 'std': 0.089}, RandomHorizontalFlip(p=1): {'mean': 0.355, 'std': 0.082}, RandomPerspective(p=0.5): {'mean': 0.618, 'std': 0.351}}\n"
     ]
    }
   ],
   "source": [
    "file_list=[\n",
    "    \"clean_images/0.png\",\n",
    "    \"clean_images/1.png\",\n",
    "    \"clean_images/2.png\",\n",
    "    \"clean_images/3.png\",\n",
    "    \"clean_images/4.png\"\n",
    "]\n",
    "\n",
    "for idx, transform in enumerate(transformations):\n",
    "    print(transform)\n",
    "    temp_list = []\n",
    "    for file in file_list:\n",
    "        image_file1 = file\n",
    "        image1 = load_image(image_file1)\n",
    "        if sim_metric == 'llava' or sim_metric == 'ssim':\n",
    "            image1_tensor = image_processor.preprocess(image1, return_tensors='pt')['pixel_values'].cuda()\n",
    "        else:\n",
    "            image1_tensor = vis_processor(image1).unsqueeze(0).to(model.device)\n",
    "        transformed_image_tensor = transform(image1_tensor).cuda()\n",
    "        sim = calculate_similarity(image1_tensor, transformed_image_tensor, sim_metric, model)\n",
    "        temp_list.append(sim)\n",
    "    mean_sim = torch.mean(torch.stack(temp_list)).item()\n",
    "    std_sim = torch.std(torch.stack(temp_list)).item()\n",
    "    sim_dict[transform] = {'mean': round(mean_sim, 3), 'std': round(std_sim, 3)}\n",
    "\n",
    "# Print the final dictionary with mean and standard deviation values\n",
    "print(sim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigpt4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
