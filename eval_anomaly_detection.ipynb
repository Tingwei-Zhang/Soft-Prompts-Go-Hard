{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"output/model\"\n",
    "constraints = {\n",
    "    \"Positive\": \"Sentiment/Positive/constrained_eps_32_batch_8\",\n",
    "    \"Negative\": \"Sentiment/Negative/constrained_eps_32_batch_8\",\n",
    "    \"Neutral\": \"Sentiment/Neutral/constrained_eps_32_batch_8\",\n",
    "    \"Formal\": \"Formality/Formal/constrained_eps_32_batch_8\",\n",
    "    \"Informal\": \"Formality/Informal/constrained_eps_32_batch_8\",\n",
    "    \"English\": \"Language/English/constrained_eps_32_batch_8\",\n",
    "    \"French\": \"Language/French/constrained_eps_32_batch_8\",\n",
    "    \"Spanish\": \"Language/Spanish/constrained_eps_32_batch_8\",\n",
    "    \"Left\": \"Politics/Left/constrained_eps_32_batch_8\",\n",
    "    \"Right\": \"Politics/Right/constrained_eps_32_batch_8\",\n",
    "    \"Spam\": \"Attack/Spam/constrained_eps_32_batch_8\",\n",
    "    \"Injection\": \"Attack/Injection/constrained_eps_32_batch_8\"\n",
    "}\n",
    "\n",
    "file_dict = {}\n",
    "for key, constraint in constraints.items():\n",
    "    # First add paths with format base_path/i/constraint/\n",
    "    paths = [f\"{base_path}/{i}/{constraint}/\" for i in range(0,5)]\n",
    "    # Then add paths with format base_path/coco_i/constraint/\n",
    "    paths.extend([f\"{base_path}/coco_{i}/{constraint}/\" for i in range(1,11)])\n",
    "    file_dict[key] = paths\n",
    "\n",
    "# change the model name to the model you want to use\n",
    "model_name=\"llava\"\n",
    "####################################\n",
    "\n",
    "if model_name == 'minigpt4':\n",
    "    for category, paths in file_dict.items():\n",
    "        file_dict[category] = [path.replace(\"model\", model_name) for path in paths]\n",
    "elif model_name == 'llava':\n",
    "    for category, paths in file_dict.items():\n",
    "        file_dict[category] = [path.replace(\"model\", model_name) for path in paths]\n",
    "elif model_name == 'blip':\n",
    "    for category, paths in file_dict.items():\n",
    "        file_dict[category] = [path.replace(\"model\", model_name) for path in paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization and Embedding Calculation\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "if model_name == \"llava\":\n",
    "    import argparse\n",
    "    from llava_llama_2.utils import get_model\n",
    "    \n",
    "    def parse_args():\n",
    "        parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "        parser.add_argument(\"--model_path\", type=str, default=\"./ckpts/llava_llama_2_13b_chat_freeze\")\n",
    "        parser.add_argument(\"--gpu_id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "        parser.add_argument(\"--model_base\", type=str, default=None)\n",
    "        return parser.parse_args(args=['--gpu_id', '0'])\n",
    "\n",
    "    args = parse_args()\n",
    "    tokenizer, model, image_processor, model_name = get_model(args)\n",
    "    model.eval()\n",
    "\n",
    "elif model_name == \"minigpt4\":\n",
    "    import argparse\n",
    "    from minigpt4.common.config import Config\n",
    "    from minigpt4.common.registry import registry\n",
    "    from minigpt_utils import prompt_wrapper, generator\n",
    "    \n",
    "    def parse_args():\n",
    "        parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "        parser.add_argument(\"--cfg-path\", default=\"eval_configs/minigpt4_eval.yaml\", help=\"path to configuration file.\")\n",
    "        parser.add_argument(\"--gpu-id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "        parser.add_argument(\"--mode\", type=str, default='VisualChatBot', choices=[\"TextOnly\", \"VisualChatBot\"])\n",
    "        parser.add_argument(\"--image_file\", type=str, default='./image.bmp', help=\"Image file\")\n",
    "        parser.add_argument(\"--output_file\", type=str, default='./result.jsonl', help=\"Output file.\")\n",
    "        parser.add_argument(\"--options\", nargs=\"+\")\n",
    "        return parser.parse_args(args=['--cfg-path', 'eval_configs/minigpt4_eval.yaml', \n",
    "                                     '--gpu-id', '0', '--mode', 'VisualChatBot', \n",
    "                                     '--image_file', 'clean_images/0.png', \n",
    "                                     '--output_file', './result.jsonl'])\n",
    "    args = parse_args()\n",
    "    cfg = Config(args)\n",
    "    model_config = cfg.model_cfg\n",
    "    model_config.device_8bit = args.gpu_id\n",
    "    model = registry.get_model_class(model_config.arch).from_config(model_config).to(f'cuda:{args.gpu_id}')\n",
    "    vis_processor = registry.get_processor_class(cfg.datasets_cfg.cc_sbu_align.vis_processor.train.name).from_config(cfg.datasets_cfg.cc_sbu_align.vis_processor.train)\n",
    "    my_generator = generator.Generator(model=model)\n",
    "\n",
    "elif model_name == \"blip\":\n",
    "    import argparse\n",
    "    from lavis.models import load_model_and_preprocess\n",
    "    \n",
    "    def parse_args():\n",
    "        parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "        parser.add_argument(\"--gpu-id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "        parser.add_argument(\"--data_path\", type=str, default=\"instruction_data/0/Sentiment/dataset.csv\")\n",
    "        parser.add_argument(\"--image_file\", type=str, default='./image.bmp', help=\"Image file\")\n",
    "        parser.add_argument(\"--output_file\", type=str, default='./result.jsonl', help=\"Output file.\")\n",
    "        parser.add_argument(\"--instruction\", type=str, default=None, choices=[\"positive\", \"negative\", \"neutral\", \"irony\", \"non_irony\", \"formal\", \"informal\", \"french\", \"english\", \"spanish\", \"left\", \"right\", \"inference_content_evaluation\", \"injection\", \"spam\"])\n",
    "        parser.add_argument(\"--image_index\", type=int, default=0)\n",
    "        return parser.parse_args(args=['--data_path', 'instruction_data/0/Attack/dataset.csv',\n",
    "                                     '--image_file', 'clean_images/0.png',\n",
    "                                     '--output_file', './result.jsonl'])\n",
    "\n",
    "    args = parse_args()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model, vis_processor, _ = load_model_and_preprocess(\n",
    "        name='blip2_vicuna_instruct',\n",
    "        model_type='vicuna13b',\n",
    "        is_eval=True,\n",
    "        device=device\n",
    "    )\n",
    "    model.eval()\n",
    "    img = Image.open(args.image_file).convert('RGB')\n",
    "    img = vis_processor[\"eval\"](img).unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity between the embeddings of perturbed inputs and their transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def denormalize(images):\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()\n",
    "    return images * std[None, :, None, None] + mean[None, :, None, None]\n",
    "\n",
    "def load_image(image_path):\n",
    "    return Image.open(image_path).convert('RGB')\n",
    "\n",
    "def calculate_similarity(image1, image2, model_name, model):\n",
    "    cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    \n",
    "    # Process images if not already tensors\n",
    "    if not isinstance(image1, torch.Tensor):\n",
    "        if model_name == 'llava':\n",
    "            image1 = image_processor.preprocess(image1, return_tensors='pt')['pixel_values'].cuda()\n",
    "            image2 = image_processor.preprocess(image2, return_tensors='pt')['pixel_values'].cuda()\n",
    "        elif model_name == 'minigpt4':\n",
    "            image1 = [vis_processor(image1).unsqueeze(0).to(model.device)]\n",
    "            image2 = [vis_processor(image2).unsqueeze(0).to(model.device)]\n",
    "        elif model_name == 'blip':\n",
    "            image1 = vis_processor[\"eval\"](image1).unsqueeze(0).to(device)\n",
    "            image2 = vis_processor[\"eval\"](image2).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get embeddings based on model type\n",
    "    if model_name == 'llava':\n",
    "        emb1 = model.encode_images(image1.half())\n",
    "        emb2 = model.encode_images(image2.half())\n",
    "    elif model_name == 'minigpt4':\n",
    "        prompt1 = prompt_wrapper.Prompt(model=model, img_prompts=[[image1]])\n",
    "        prompt2 = prompt_wrapper.Prompt(model=model, img_prompts=[[image2]])\n",
    "        emb1, emb2 = prompt1.img_embs[0][0], prompt2.img_embs[0][0]\n",
    "    elif model_name == 'blip':\n",
    "        with model.maybe_autocast():\n",
    "            emb1 = model.ln_vision(model.visual_encoder(image1))\n",
    "            emb2 = model.ln_vision(model.visual_encoder(image2))\n",
    "            \n",
    "    return cos(emb1.view(-1).to(torch.float32), emb2.view(-1).to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "sim_dict = {}\n",
    "\n",
    "def jpeg_transform(X):\n",
    "    save_image(torch.squeeze(denormalize(X)), 'dummy.jpg')\n",
    "    image = load_image('dummy.jpg')\n",
    "    if model_name == 'blip':\n",
    "        return vis_processor[\"eval\"](image).unsqueeze(0).cuda()\n",
    "    elif model_name =='llava':\n",
    "        return image_processor.preprocess(image, return_tensors='pt')['pixel_values'].cuda()\n",
    "    elif model_name == 'minigpt4':\n",
    "        return vis_processor(image).unsqueeze(0).cuda()\n",
    "\n",
    "transformations = [\n",
    "    jpeg_transform,\n",
    "    transforms.GaussianBlur(kernel_size=9, sigma=(1.0, 5.0)),\n",
    "    transforms.RandomAffine(degrees=45),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.2),\n",
    "    transforms.RandomHorizontalFlip(p=1),\n",
    "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5)\n",
    "]\n",
    "\n",
    "for transform in transformations:\n",
    "    similarities = []\n",
    "    for key, files in file_dict.items():\n",
    "        for file in files:\n",
    "            image = load_image(file + \"bad_prompt.bmp\")\n",
    "            image_tensor = (vis_processor[\"eval\"](image).unsqueeze(0).to(device) if model_name == 'blip'\n",
    "                          else image_processor.preprocess(image, return_tensors='pt')['pixel_values'].cuda() if model_name == 'llava'\n",
    "                          else vis_processor(image).unsqueeze(0).to(model.device))\n",
    "            transformed = transform(image_tensor).cuda()\n",
    "            similarities.append(calculate_similarity(image_tensor, transformed, model_name, model))\n",
    "    \n",
    "    sims = torch.stack(similarities)\n",
    "    sim_dict[transform] = {\n",
    "        'mean': round(torch.mean(sims).item(), 3),\n",
    "        'std': round(torch.std(sims).item(), 3)\n",
    "    }\n",
    "\n",
    "print(sim_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Cosine similarity between the embeddings of unperturbed inputs and their transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [f\"clean_images/coco_{i}.jpg\" for i in range(1, 11)]\n",
    "\n",
    "for transform in transformations:\n",
    "    similarities = []\n",
    "    for file in file_list:\n",
    "        image = load_image(file)\n",
    "        image_tensor = (vis_processor[\"eval\"](image).unsqueeze(0).to(device) if model_name == 'blip'\n",
    "                       else image_processor.preprocess(image, return_tensors='pt')['pixel_values'].cuda() if model_name in ['llava', 'ssim']\n",
    "                       else vis_processor(image).unsqueeze(0).to(model.device))\n",
    "        transformed = transform(image_tensor).cuda()\n",
    "        similarities.append(calculate_similarity(image_tensor, transformed, model_name, model))\n",
    "    \n",
    "    sims = torch.stack(similarities)\n",
    "    sim_dict[transform] = {\n",
    "        'mean': round(torch.mean(sims).item(), 3),\n",
    "        'std': round(torch.std(sims).item(), 3)\n",
    "    }\n",
    "\n",
    "print(sim_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigpt4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
