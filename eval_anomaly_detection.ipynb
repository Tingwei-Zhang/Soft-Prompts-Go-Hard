{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"output/model\"\n",
    "constraints = {\n",
    "    \"Baseline\": \"baseline_1\",\n",
    "    \"Positive\": \"Sentiment/Positive/constrained_eps_32_batch_8\",\n",
    "    \"Negative\": \"Sentiment/Negative/constrained_eps_32_batch_8\",\n",
    "    \"Neutral\": \"Sentiment/Neutral/constrained_eps_32_batch_8\",\n",
    "    \"Formal\": \"Formality/Formal/constrained_eps_32_batch_8\",\n",
    "    \"Informal\": \"Formality/Informal/constrained_eps_32_batch_8\",\n",
    "    \"English\": \"Language/English/constrained_eps_32_batch_8\",\n",
    "    \"French\": \"Language/French/constrained_eps_32_batch_8\",\n",
    "    \"Spanish\": \"Language/Spanish/constrained_eps_32_batch_8\",\n",
    "    \"Left\": \"Politics/Left/constrained_eps_32_batch_8\",\n",
    "    \"Right\": \"Politics/Right/constrained_eps_32_batch_8\",\n",
    "    \"Spam\": \"Attack/Spam/constrained_eps_32_batch_8\",\n",
    "    \"Injection\": \"Attack/Injection/constrained_eps_32_batch_8\"\n",
    "}\n",
    "\n",
    "file_dict = {}\n",
    "for key, constraint in constraints.items():\n",
    "    # First add paths with format base_path/i/constraint/\n",
    "    paths = [f\"{base_path}/{i}/{constraint}/\" for i in range(0,5)]\n",
    "    # Then add paths with format base_path/coco_i/constraint/\n",
    "    paths.extend([f\"{base_path}/coco_{i}/{constraint}/\" for i in range(1,11)])\n",
    "    file_dict[key] = paths\n",
    "\n",
    "model_name=\"minigpt4\"\n",
    "if model_name == 'minigpt4':\n",
    "    for category, paths in file_dict.items():\n",
    "        file_dict[category] = [path.replace(\"model\", model_name) for path in paths]\n",
    "elif model_name == 'llava':\n",
    "    for category, paths in file_dict.items():\n",
    "        file_dict[category] = [path.replace(\"model\", model_name) for path in paths]\n",
    "elif model_name == 'blip':\n",
    "    for category, paths in file_dict.items():\n",
    "        file_dict[category] = [path.replace(\"model\", model_name) for path in paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity between oringinal and perturbed image\n",
    "## LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"./ckpts/llava_llama_2_13b_chat_freeze\")\n",
    "    parser.add_argument(\"--gpu_id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--model_base\", type=str, default=None)\n",
    "    args = parser.parse_args(args=['--gpu_id', '0'])\n",
    "    return args\n",
    "\n",
    "# ========================================\n",
    "#             Model Initialization\n",
    "# ========================================\n",
    "print('>>> Initializing Models')\n",
    "from llava_llama_2.utils import get_model\n",
    "args = parse_args()\n",
    "print('model = ', args.model_path)\n",
    "tokenizer, model, image_processor, model_name = get_model(args)\n",
    "model.eval()\n",
    "print('[Initialization Finished]\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minigpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "# import gradio as gr\n",
    "from PIL import Image\n",
    "\n",
    "from minigpt4.common.config import Config\n",
    "from minigpt4.common.dist_utils import get_rank\n",
    "from minigpt4.common.registry import registry\n",
    "\n",
    "# imports modules for registration\n",
    "from minigpt4.datasets.builders import *\n",
    "from minigpt4.models import *\n",
    "from minigpt4.processors import *\n",
    "from minigpt4.runners import *\n",
    "from minigpt4.tasks import *\n",
    "\n",
    "from minigpt_utils import prompt_wrapper, generator\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "\n",
    "    parser.add_argument(\"--cfg-path\", default=\"eval_configs/minigpt4_eval.yaml\", help=\"path to configuration file.\")\n",
    "    parser.add_argument(\"--gpu-id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--mode\", type=str, default='VisualChatBot',\n",
    "                        choices=[ \"TextOnly\", \"VisualChatBot\" ],\n",
    "                        help=\"Inference Mode: TextOnly: Text model only (Vicuna) \\n VisualChatBot: Vision model + Text model (MiniGPT4) \")\n",
    "    parser.add_argument(\"--image_file\", type=str, default='./image.bmp',\n",
    "                        help=\"Image file\")\n",
    "    parser.add_argument(\"--output_file\", type=str, default='./result.jsonl',\n",
    "                        help=\"Output file.\")\n",
    "    parser.add_argument(\n",
    "        \"--options\",\n",
    "        nargs=\"+\",\n",
    "        help=\"override some settings in the used config, the key-value pair \"\n",
    "        \"in xxx=yyy format will be merged into config file (deprecate), \"\n",
    "        \"change to --cfg-options instead.\",\n",
    "    )\n",
    "    args = parser.parse_args(args=['--cfg-path', 'eval_configs/minigpt4_eval.yaml', \n",
    "                               '--gpu-id', '0', '--mode', 'VisualChatBot', '--image_file', 'clean_images/0.png', '--output_file', './result.jsonl'])\n",
    "    # args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def setup_seeds(config):\n",
    "    seed = config.run_cfg.seed + get_rank()\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "\n",
    "# ========================================\n",
    "#             Model Initialization\n",
    "# ========================================\n",
    "\n",
    "print('>>> Initializing Models')\n",
    "args = parse_args()\n",
    "cfg = Config(args)\n",
    "model_config = cfg.model_cfg\n",
    "model_config.device_8bit = args.gpu_id\n",
    "model_cls = registry.get_model_class(model_config.arch)\n",
    "model = model_cls.from_config(model_config).to('cuda:{}'.format(args.gpu_id))\n",
    "vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\n",
    "vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "print('Initialization Finished')\n",
    "my_generator = generator.Generator(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "    parser.add_argument(\"--gpu-id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"instruction_data/0/Sentiment/dataset.csv\")\n",
    "    parser.add_argument(\"--image_file\", type=str, default='./image.bmp',\n",
    "                        help=\"Image file\")\n",
    "    parser.add_argument(\"--output_file\", type=str, default='./result.jsonl',\n",
    "                        help=\"Output file.\")\n",
    "    parser.add_argument(\"--instruction\", type=str, default=None,\n",
    "                    choices=[ \"positive\", \"negative\", \"neutral\", \"irony\", \"non_irony\", \"formal\", \"informal\", \"french\", \"english\", \"spanish\", \"left\",\"right\",\"inference_content_evaluation\",\"injection\",\"spam\"],\n",
    "                        help=\"Instruction to be used for the attack.\")\n",
    "    parser.add_argument(\"--image_index\", type=int, default=0)\n",
    "    # args = parser.parse_args()\n",
    "    args = parser.parse_args(args=['--data_path', 'instruction_data/0/Attack/dataset.csv', \n",
    "                                '--image_file', 'clean_images/0.png', '--output_file', './result.jsonl'])\n",
    "    return args\n",
    "\n",
    "\n",
    "# ========================================\n",
    "#             Model Initialization\n",
    "# ========================================\n",
    "\n",
    "print('>>> Initializing Models')\n",
    "\n",
    "from lavis.models import load_model_and_preprocess\n",
    "\n",
    "args = parse_args()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# remember to modify the parameter llm_model in ./lavis/configs/models/blip2/blip2_instruct_vicuna13b.yaml to the path that store the vicuna weights\n",
    "model, vis_processor, _ = load_model_and_preprocess(\n",
    "        name='blip2_vicuna_instruct',\n",
    "        model_type='vicuna13b',\n",
    "        is_eval=True,\n",
    "        device=device,\n",
    "    )\n",
    "model.eval()\n",
    "\"\"\"\n",
    "Source code of the model in:\n",
    "    ./lavis/models/blip2_models/blip2_vicuna_instruct.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "img = Image.open(args.image_file).convert('RGB')\n",
    "img = vis_processor[\"eval\"](img).unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pytorch_msssim import ssim\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def denormalize(images):\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()\n",
    "    images = images * std[None, :, None, None]\n",
    "    images = images + mean[None, :, None, None]\n",
    "    return images\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    return image\n",
    "\n",
    "def calculate_similarity(image1, image2, metric, model):\n",
    "    if metric=='llava':\n",
    "        cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "        if not isinstance(image1, torch.Tensor):\n",
    "            image1 = image_processor.preprocess(image1, return_tensors='pt')['pixel_values'].cuda()\n",
    "            image2 = image_processor.preprocess(image2, return_tensors='pt')['pixel_values'].cuda()\n",
    "        emb_image1 = model.encode_images(image1.half())\n",
    "        emb_image2 = model.encode_images(image2.half())\n",
    "        cos_sim = cos(emb_image1.view(-1).to(torch.float32), emb_image2.view(-1).to(torch.float32))\n",
    "        return cos_sim\n",
    "    elif metric=='minigpt4':\n",
    "        cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "        if not isinstance(image1, torch.Tensor):\n",
    "            image1 = [vis_processor(image1).unsqueeze(0).to(model.device)]\n",
    "            image2 = [vis_processor(image2).unsqueeze(0).to(model.device)]\n",
    "        else:\n",
    "            image1=[image1]\n",
    "            image2=[image2]\n",
    "        prompt1 = prompt_wrapper.Prompt(model=model, img_prompts=[image1])\n",
    "        emb_image1 = prompt1.img_embs[0][0]\n",
    "        prompt2 = prompt_wrapper.Prompt(model=model, img_prompts=[image2])\n",
    "        emb_image2 = prompt2.img_embs[0][0]\n",
    "        cos_sim = cos(emb_image1.view(-1).to(torch.float32), emb_image2.view(-1).to(torch.float32))\n",
    "        return cos_sim\n",
    "    elif metric=='blip':\n",
    "        cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "        if not isinstance(image1, torch.Tensor):\n",
    "            image1 = vis_processor[\"eval\"](image1).unsqueeze(0).to(device)\n",
    "            image2 = vis_processor[\"eval\"](image2).unsqueeze(0).to(device)\n",
    "        with model.maybe_autocast():\n",
    "            emb_image1 = model.ln_vision(model.visual_encoder(image1))\n",
    "            emb_image2 = model.ln_vision(model.visual_encoder(image2))\n",
    "        cos_sim = cos(emb_image1.view(-1).to(torch.float32), emb_image2.view(-1).to(torch.float32))\n",
    "        return cos_sim\n",
    "\n",
    "def calculate_transformations_sim(image, metric, model, show_plots=True):\n",
    "\n",
    "    transformations = [\n",
    "        transforms.GaussianBlur(kernel_size=9, sigma=(1.0, 5.0)),\n",
    "        transforms.RandomAffine(degrees=45),  # Rotate by 45 degrees\n",
    "        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.2),\n",
    "        transforms.RandomHorizontalFlip(p=1),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5, p=1)\n",
    "    ]\n",
    "    \n",
    "    # image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].cuda()\n",
    "    # for blip\n",
    "    image_tensor = vis_processor[\"eval\"](image).unsqueeze(0).to(device)\n",
    "    if show_plots:\n",
    "        fig, axs = plt.subplots(1, len(transformations) + 1, figsize=(15, 10))  # +1 for the original image\n",
    "        axs[0].imshow(denormalize(image_tensor)[0].detach().cpu().permute(1, 2, 0))\n",
    "        axs[0].set_title('Original')\n",
    "        axs[0].axis('off')\n",
    "\n",
    "    sim_list = []\n",
    "    for idx, transform in enumerate(transformations):\n",
    "        transformed_image_tensor = transform(image_tensor)  # Apply transformation\n",
    "        if show_plots:\n",
    "            axs[idx + 1].imshow(denormalize(transformed_image_tensor)[0].detach().cpu().permute(1, 2, 0))\n",
    "            axs[idx + 1].set_title(transform.__class__.__name__)\n",
    "            axs[idx + 1].axis('off')\n",
    "        if metric=='ssim':\n",
    "            sim = calculate_similarity(denormalize(image_tensor), denormalize(transformed_image_tensor),model, metric)\n",
    "        else:\n",
    "            sim = calculate_similarity(denormalize(image_tensor), denormalize(transformed_image_tensor),model, metric)\n",
    "        sim_list.append(sim)\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "        \n",
    "    return sum(sim_list)/len(sim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "sim_dict={}\n",
    "sim_metric='llava'\n",
    "temp_list=[]\n",
    "\n",
    "def jpeg_transform(X):\n",
    "    image_filename = 'dummy.jpg'\n",
    "    save_image(torch.squeeze(denormalize(X)), image_filename)\n",
    "    image = load_image(image_filename)\n",
    "    if sim_metric=='blip':\n",
    "        jpeg_X = vis_processor[\"eval\"](image).unsqueeze(0).to(device)\n",
    "    if sim_metric == 'llava' or sim_metric == 'ssim':\n",
    "        jpeg_X = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].cuda()\n",
    "    else:\n",
    "        jpeg_X = vis_processor(image).unsqueeze(0).to(model.device)\n",
    "    jpeg_X=jpeg_X.clamp(0,1)\n",
    "    return jpeg_X.cpu()\n",
    "\n",
    "# Define a list of transformations to apply\n",
    "transformations = [\n",
    "    jpeg_transform,\n",
    "    transforms.GaussianBlur(kernel_size=9, sigma=(1.0, 5.0)),\n",
    "    transforms.RandomAffine(degrees=45),  # Rotate by 45 degrees\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.2),\n",
    "    transforms.RandomHorizontalFlip(p=1),\n",
    "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5)\n",
    "]\n",
    "\n",
    "\n",
    "for idx, transform in enumerate(transformations):\n",
    "    print(transform)\n",
    "    temp_list = []\n",
    "    for key in file_dict:\n",
    "        for file in file_dict[key]:\n",
    "            image_file1 = file + \"bad_prompt.bmp\"\n",
    "            image1 = load_image(image_file1)\n",
    "            if sim_metric=='blip':\n",
    "                jpeg_X = vis_processor[\"eval\"](image1).unsqueeze(0).to(device)\n",
    "            if sim_metric == 'llava' or sim_metric == 'ssim':\n",
    "                image1_tensor = image_processor.preprocess(image1, return_tensors='pt')['pixel_values'].cuda()\n",
    "            else:\n",
    "                image1_tensor = vis_processor(image1).unsqueeze(0).to(model.device)\n",
    "            transformed_image_tensor = transform(image1_tensor).cuda()\n",
    "            sim = calculate_similarity(image1_tensor, transformed_image_tensor, sim_metric, model)\n",
    "            temp_list.append(sim)\n",
    "    mean_sim = torch.mean(torch.stack(temp_list)).item()\n",
    "    std_sim = torch.std(torch.stack(temp_list)).item()\n",
    "    sim_dict[transform] = {'mean': round(mean_sim, 3), 'std': round(std_sim, 3)}\n",
    "\n",
    "# Print the final dictionary with mean and standard deviation values\n",
    "print(sim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=[\n",
    "    \"clean_images/coco_1.jpg\",\n",
    "    \"clean_images/coco_2.jpg\",\n",
    "    \"clean_images/coco_3.jpg\",\n",
    "    \"clean_images/coco_4.jpg\",\n",
    "    \"clean_images/coco_5.jpg\",\n",
    "    \"clean_images/coco_6.jpg\",\n",
    "    \"clean_images/coco_7.jpg\",\n",
    "    \"clean_images/coco_8.jpg\",\n",
    "    \"clean_images/coco_9.jpg\",\n",
    "    \"clean_images/coco_10.jpg\"\n",
    "]\n",
    "\n",
    "for idx, transform in enumerate(transformations):\n",
    "    print(transform)\n",
    "    temp_list = []\n",
    "    for file in file_list:\n",
    "        image_file1 = file\n",
    "        image1 = load_image(image_file1)\n",
    "        if sim_metric=='blip':\n",
    "            image1_tensor = vis_processor[\"eval\"](image1).unsqueeze(0).to(device)\n",
    "        if sim_metric == 'llava' or sim_metric == 'ssim':\n",
    "            image1_tensor = image_processor.preprocess(image1, return_tensors='pt')['pixel_values'].cuda()\n",
    "        else:\n",
    "            image1_tensor = vis_processor(image1).unsqueeze(0).to(model.device)\n",
    "        transformed_image_tensor = transform(image1_tensor).cuda()\n",
    "        sim = calculate_similarity(image1_tensor, transformed_image_tensor, sim_metric, model)\n",
    "        temp_list.append(sim)\n",
    "    mean_sim = torch.mean(torch.stack(temp_list)).item()\n",
    "    std_sim = torch.std(torch.stack(temp_list)).item()\n",
    "    sim_dict[transform] = {'mean': round(mean_sim, 3), 'std': round(std_sim, 3)}\n",
    "\n",
    "# Print the final dictionary with mean and standard deviation values\n",
    "print(sim_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigpt4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
